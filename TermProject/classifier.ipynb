{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDir(name,stemming,lower_case):\n",
    "    # Loads the files in the folder and returns a list of lists of words from the text in each file\n",
    "    if stemming:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    X0 = []\n",
    "    count = 0\n",
    "    for f in tqdm(listdir(name)):\n",
    "        fullname = name+f\n",
    "        text = []\n",
    "        with open(fullname, 'rb') as f:\n",
    "            for line in f:\n",
    "                if lower_case:\n",
    "                    line = line.decode(errors='ignore').lower()\n",
    "                    text += tokenizer.tokenize(line)\n",
    "                else:\n",
    "                    text += tokenizer.tokenize(line.decode(errors='ignore'))\n",
    "        if stemming:\n",
    "            for i in range(len(text)):\n",
    "#                 if text[i] in bad_words:\n",
    "#                     continue\n",
    "                text[i] = porter_stemmer.stem(text[i])\n",
    "        X0.append(text)\n",
    "        count = count + 1\n",
    "    return X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(directory, stemming, lower_case):\n",
    "    positive = loadDir(directory + '/pos/',stemming, lower_case)\n",
    "    negative = loadDir(directory + '/neg/',stemming, lower_case)\n",
    "    random.shuffle(positive)\n",
    "    boundaryTrain = math.floor(0.8 * len(positive))\n",
    "    trainPos = positive[:boundaryTrain]\n",
    "    random.shuffle(negative)\n",
    "    trainNeg = negative[:boundaryTrain]\n",
    "    combinedTrain = trainPos + trainNeg\n",
    "    length = len(trainPos) + len(trainNeg)\n",
    "    labelsTrain = len(trainNeg) * [1] + len(trainNeg) * [0]\n",
    "    labelsTrain = np.array(labelsTrain)\n",
    "\n",
    "    testPos = positive[boundaryTrain:]\n",
    "    testNeg = negative[boundaryTrain:]\n",
    "    combinedTest = testPos + testNeg\n",
    "    labelsTest = len(testPos) * [1] + len(testNeg) * [0]\n",
    "    labelsTest = np.array(labelsTest)\n",
    "    print(labelsTrain)\n",
    "    return combinedTrain, labelsTrain, combinedTest, labelsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "    #Baseline#\n",
    "    # return predicted labels of development set\n",
    "    # print(\"not even started yet\")\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    mydict = {}\n",
    "    smoothing_parameter = 0.034\n",
    "    posV = 0\n",
    "    negV = 0\n",
    "    totalposwords = 0\n",
    "    totalnegwords = 0\n",
    "\n",
    "    start = time.process_time()\n",
    "#     print(\"creating occurences and wordlist\")\n",
    "\n",
    "    #create bag of words and number of occurences\n",
    "    count = 0\n",
    "    for x in train_set:\n",
    "        rating = train_labels[count]\n",
    "        count += 1\n",
    "        if(rating):\n",
    "            for y in x:\n",
    "                if y not in mydict and y not in stops:\n",
    "                    mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "                    posV += 1\n",
    "                    totalposwords += 1\n",
    "                elif y not in stops:\n",
    "                    if mydict[y][0] == 0:\n",
    "                        posV += 1\n",
    "                    mydict[y][0] += 1\n",
    "                    totalposwords += 1\n",
    "        else:\n",
    "            for y in x:\n",
    "                if y not in mydict and y not in stops:\n",
    "                    mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "                    negV += 1\n",
    "                    totalnegwords += 1\n",
    "                elif y not in stops:\n",
    "                    if mydict[y][1] == 0:\n",
    "                        negV += 1\n",
    "                    mydict[y][1] += 1\n",
    "                    totalnegwords += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "#     print(\"Going through train set took: \", time.process_time() - start)\n",
    "\n",
    "    \n",
    "    #come up with the bag of words unigram model\n",
    "    probWordPos = {}\n",
    "    probWordNeg = {}\n",
    "    # PosII = 0\n",
    "    # NegII = 0\n",
    "    start = time.process_time()\n",
    "#     print(\"calculate prob\")\n",
    "    for x in mydict:\n",
    "        #use laplace smoothing\n",
    "        # count(W) + a / n + a * (V+1)\n",
    "        # n = number of words in our UK training data\n",
    "        # count(W) = number of times W appeared in UK training data\n",
    "        # Î± is a tuning constant between 0 and 1 (typically small)\n",
    "        # V = number of word TYPES seen in training data\n",
    "\n",
    "        probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "        probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        #calculate that II symbol that is basically summation, but using mutiplication\n",
    "        #logs are used because we are working with incredibly small numbers\n",
    "        # PosII += (probWordPos[x])\n",
    "        # NegII += (probWordNeg[x])\n",
    "    \n",
    "\n",
    "    #unneccessary calculations on prob of train set\n",
    "#     print(\"Prob calculations: \", time.process_time() - start)\n",
    "    # print(\"PosII\", PosII)\n",
    "    # print(\"NegII\", NegII)\n",
    "    # probPos = math.log(pos_prior) + PosII\n",
    "    # probNeg = math.log(1 - pos_prior) + NegII\n",
    "\n",
    "    # print(\"positive\", probPos)\n",
    "    # print(\"negative\", probNeg)\n",
    "\n",
    "\n",
    "    start = time.process_time()\n",
    "    # #multiply by (add log) the pos prior, which is the other part of our equation in the unigram model\n",
    "    # time to work with the dev set\n",
    "    predictions = []\n",
    "    for x in range(len(dev_set)):\n",
    "        chancePos = math.log(pos_prior)\n",
    "        chanceNeg = math.log(1-pos_prior)\n",
    "        for y in range(len(dev_set[x])):\n",
    "            if dev_set[x][y] in mydict:\n",
    "                chancePos += probWordPos[dev_set[x][y]]\n",
    "                chanceNeg += probWordNeg[dev_set[x][y]]\n",
    "            # else:\n",
    "                # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "                # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        if(chancePos > chanceNeg):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Mixed Model approach (unigram+bigram) ###################\n",
    "# def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "#     \"\"\"\n",
    "#     train_set - List of list of words corresponding with each movie review\n",
    "#     example: suppose I had two reviews 'like this movie' and 'i fall asleep' in my training set\n",
    "#     Then train_set := [['like','this','movie'], ['i','fall','asleep']]\n",
    "\n",
    "#     train_labels - List of labels corresponding with train_set\n",
    "#     example: Suppose I had two reviews, first one was positive and second one was negative.\n",
    "#     Then train_labels := [1, 0]\n",
    "\n",
    "#     dev_set - List of list of words corresponding with each review that we are testing on\n",
    "#               It follows the same format as train_set\n",
    "\n",
    "#     smoothing_parameter - The smoothing parameter you provided with --laplace (1.0 by default)\n",
    "#     \"\"\"\n",
    "#     begin = time.process_time()\n",
    "#     stops = stopwords.words('english') + list(string.punctuation)\n",
    "#     mydict = {}\n",
    "#     mybidict = {}\n",
    "#     smoothing_parameter = 0.21\n",
    "#     smoothing_parameter_bi = 0.75\n",
    "#     posV = 0\n",
    "#     posVbi = 0\n",
    "#     negV = 0\n",
    "#     posVbi = 0\n",
    "#     negVbi = 0\n",
    "#     totalposwords = 0\n",
    "#     totalposwordsbi = 0\n",
    "#     totalnegwords = 0\n",
    "#     totalnegwordsbi = 0\n",
    "\n",
    "#     print(\"creating occurences and wordlist\")\n",
    "#     #create bag of words and number of occurences\n",
    "\n",
    "#     start = time.process_time()\n",
    "#     count = 0\n",
    "#     for x in train_set:\n",
    "#         rating = train_labels[count]\n",
    "#         count += 1\n",
    "#         if(rating):\n",
    "#             for y in x:\n",
    "#                 if y in stops:\n",
    "#                     continue\n",
    "#                 if y not in mydict:\n",
    "#                     mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "#                     posV += 1\n",
    "#                     totalposwords += 1\n",
    "#                 else:\n",
    "#                     if mydict[y][0] == 0:\n",
    "#                         posV += 1\n",
    "#                     mydict[y][0] += 1\n",
    "#                     totalposwords += 1\n",
    "#             for y,z in zip(x, x[1:]):\n",
    "#                 if y in stops or z in stops:\n",
    "#                     continue\n",
    "#                 if (y,z) not in mybidict:\n",
    "#                     mybidict[(y,z)] = [1,0]\n",
    "#                     posVbi += 1\n",
    "#                     totalposwordsbi += 1\n",
    "#                 else:\n",
    "#                     if mybidict[(y,z)][0] == 0:\n",
    "#                         posVbi += 1\n",
    "#                     mybidict[(y,z)][0] += 1\n",
    "#                     totalposwordsbi += 1\n",
    "#                     # print (\"[x,y] : \", [x,y])\n",
    "#         else:\n",
    "#             for y in x:\n",
    "#                 if y in stops:\n",
    "#                     continue\n",
    "#                 if y not in mydict:\n",
    "#                     mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "#                     negV += 1\n",
    "#                     totalnegwords += 1\n",
    "#                 else:\n",
    "#                     if mydict[y][1] == 0:\n",
    "#                         negV += 1\n",
    "#                     mydict[y][1] += 1\n",
    "#                     totalnegwords += 1\n",
    "#             for y,z in zip(x, x[1:]):\n",
    "#                 if y in stops or z in stops:\n",
    "#                     continue\n",
    "#                 if (y,z) not in mybidict:\n",
    "#                     # print(\"(\",y,\",\", z, \")\")\n",
    "#                     mybidict[(y,z)] = [0,1]\n",
    "#                     negVbi += 1\n",
    "#                     totalnegwordsbi += 1\n",
    "#                 else:\n",
    "#                     if mybidict[(y,z)][1] == 0:\n",
    "#                         negVbi += 1\n",
    "#                     mybidict[(y,z)][1] += 1\n",
    "#                     totalnegwordsbi += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "#     print(\"posVbi\", posVbi)\n",
    "#     print(\"negVbi\", negVbi)\n",
    "#     print(\"total bi pair count is:\", totalposwordsbi  + totalnegwordsbi)\n",
    "#     print(\"Going through train took: \", time.process_time() - start)\n",
    "\n",
    "    \n",
    "#     #come up with the bag of words\n",
    "#     probWordPos = {}\n",
    "#     probWordNeg = {}\n",
    "#     # PosII = 0\n",
    "#     # NegII = 0\n",
    "#     # print(\"calculate prob\")\n",
    "#     for x in mydict:\n",
    "#         #use laplace smoothing\n",
    "#         # count(W) + a / n + a * (V+1)\n",
    "#         # n = number of words in our UK training data\n",
    "#         # count(W) = number of times W appeared in UK training data\n",
    "#         # Î± is a tuning constant between 0 and 1 (typically small)\n",
    "#         # V = number of word TYPES seen in training data\n",
    "\n",
    "#         probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "#         probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "#         #calculate that II symbol that is basically summation, but using mutiplication\n",
    "#         #logs are used because we are working with incredibly small numbers\n",
    "#         # PosII += (probWordPos[x])\n",
    "#         # NegII += (probWordNeg[x])\n",
    "#     # start = time.process_time()\n",
    "#     probPairPos = {}\n",
    "#     probPairNeg = {}\n",
    "#     for x in mybidict:\n",
    "#         probPairPos[x] = math.log((mybidict[x][0] + smoothing_parameter_bi) / (totalposwordsbi + smoothing_parameter_bi * (posVbi + 1)))\n",
    "#         probPairNeg[x] = math.log((mybidict[x][1] + smoothing_parameter_bi) / (totalnegwordsbi + smoothing_parameter_bi * (negVbi + 1)))\n",
    "#     #unneccessary calculations on prob of train set\n",
    "#     # print(\"time to print bi dict\", time.process_time() - start)\n",
    "#     # print(\"PosII\", PosII)\n",
    "#     # print(\"NegII\", NegII)\n",
    "#     # probPos = math.log(pos_prior) + PosII\n",
    "#     # probNeg = math.log(1 - pos_prior) + NegII\n",
    "\n",
    "#     # print(\"positive\", probPos)\n",
    "#     # print(\"negative\", probNeg)\n",
    "    \n",
    "#     start = time.process_time()\n",
    "#     # #multiply by (add log) the pos prior, which is the other part of our equation in the unigram    model\n",
    "#     # time to work with the dev set\n",
    "#     predictions = []\n",
    "#     lambd = 0.475\n",
    "#     lambdaUni = lambd\n",
    "#     lambdaBi = 1 - lambd\n",
    "#     for x in range(len(dev_set)):\n",
    "#         chancePosUni = math.log(pos_prior)\n",
    "#         chanceNegUni = math.log(1-pos_prior)\n",
    "#         chancePosBi = math.log(pos_prior)\n",
    "#         chanceNegBi = math.log(1-pos_prior)\n",
    "#         for y in range(len(dev_set[x])):\n",
    "#             if dev_set[x][y] in mydict:\n",
    "#                 chancePosUni += probWordPos[dev_set[x][y]]\n",
    "#                 chanceNegUni += probWordNeg[dev_set[x][y]]\n",
    "#             # else:\n",
    "#                 # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "#                 # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "#         for y,z in zip(dev_set[x], dev_set[x][1:]):\n",
    "#             if (y,z) in mybidict:\n",
    "#                 chancePosBi += probPairPos[(y,z)]\n",
    "#                 chanceNegBi += probPairNeg[(y,z)]\n",
    "#         chancePos = (lambdaBi * chancePosBi) + (lambdaUni * chancePosUni)\n",
    "#         chanceNeg = (lambdaBi * chanceNegBi) + (lambdaUni * chanceNegUni)\n",
    "#         if(chancePos > chanceNeg):\n",
    "#             predictions.append(1)\n",
    "#         else:\n",
    "#             predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "#     print(\"method took:\", time.process_time() - begin)\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(train_set, train_labels, dev_set):\n",
    "    LRclassifier = LogisticRegression()\n",
    "    labelSet = []\n",
    "    for i in range(len(train_labels)):\n",
    "        labelSet = fillTrainset(train_set[i],train_labels[i])\n",
    "    print(labelSet)\n",
    "    print(train_set)\n",
    "    LRclassifier.fit(train_set,labelSet)\n",
    "    predictions = LRclassifier.predict(dev_set)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportVectorMachine(train_set, train_labels, dev_set):\n",
    "    SVMclassifier = LinearSVC()\n",
    "    SVMclassifier.fit(train_set,train_labels)\n",
    "    predictions = SVMclassifier.predict(dev_set)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree(train_set, train_labels, dev_set):\n",
    "    DTclassifier = DecisionTreeClassifier()\n",
    "    DTclassifier.fit(train_set,train_labels)\n",
    "    predictions = DTclassifier.predict(dev_set)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(predictedLabels, dev_set, dev_labels):\n",
    "    yhats = predictedLabels\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    tp = np.sum([yhats[i] == dev_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    precision = tp / np.sum([yhats[i] == 1 for i in range(len(yhats))])\n",
    "    recall = tp / (np.sum([yhats[i] != dev_labels[i] and yhats[i] == 0 for i in range(len(yhats))]) + tp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTrainset(review, label):\n",
    "    labelSet = []\n",
    "    if label == 0:\n",
    "        labelSet = [0]*len(review)\n",
    "    if label == 1:\n",
    "        labelSet = [1]*len(review)\n",
    "    return labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, stemming, lowerCase, laplace, posPrior):\n",
    "    trainSet, trainLabels, revSet, revLabels = loadDataset(dataset, stemming, lowerCase)\n",
    "    print(len(trainSet))\n",
    "    print(len(trainLabels))\n",
    "    print(trainSet[0])\n",
    "    print(trainSet[1])\n",
    "    print(trainLabels[0])\n",
    "    print(trainLabels[1])\n",
    "    predictedLabelsNB = naiveBayes(trainSet, trainLabels, revSet, laplace, posPrior)\n",
    "    predictedLabelsLR = logisticRegression(trainSet, trainLabels, revSet)\n",
    "    predictedLabelsSVM = supportVectorMachine(trainSet, trainLabels, revSet)\n",
    "    predictedLabelsDT = decisionTree(trainSet, trainLabels, revSet)\n",
    "    \n",
    "    accuracyNB, f1NB, precisionNB, recallNB = compute_accuracies(predictedLabelsNB, revSet, revLabels)\n",
    "    accuracyLR, f1LR, precisionLR, recallLR = compute_accuracies(predictedLabelsLR, revSet, revLabels)\n",
    "    accuracySVM, f1SVM, precisionSVM, recallSVM = compute_accuracies(predictedLabelsSVM, revSet, revLabels)\n",
    "    accuracyDT, f1DT, precisionDT, recallDT = compute_accuracies(predictedLabelsDT, revSet, revLabels)\n",
    "    \n",
    "    NBscores = accuracyNB, f1NB, precisionNB, recallNB\n",
    "    LRscores = accuracyLR, f1LR, precisionLR, recallLR\n",
    "    SVMscores = accuracySVM, f1SVM, precisionSVM, recallSVM\n",
    "    DTscores = accuracyDT, f1DT, precisionDT, recallDT\n",
    "#     print(\"Accuracy:\",accuracy)\n",
    "#     print(\"F1-Score:\",f1)\n",
    "#     print(\"Precision:\",precision)\n",
    "#     print(\"Recall:\",recall)\n",
    "    return NBscores, LRscores, SVMscores, DTscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 1000/1000 [00:00<00:00, 1679.53it/s]\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 1000/1000 [00:00<00:00, 1843.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n",
      "1600\n",
      "1600\n",
      "['plot', 'a', 'dude', 'and', 'his', 'brother', 'are', 'driving', 'cross', 'country', 'and', 'decide', 'to', 'fool', 'around', 'with', 'a', 'trucker', 'on', 'their', 'cb', 'radio', 'it', 'isn', 't', 'long', 'before', 'their', 'little', 'prank', 'gets', 'someone', 'put', 'into', 'a', 'coma', 'long', 'story', 'and', 'the', 'next', 'thing', 'you', 'know', 'the', 'trucker', 'is', 'following', 'them', 'too', 'lotsa', 'nuttiness', 'ensues', 'and', 'then', 'they', 'pick', 'up', 'their', 'other', 'friend', 'venna', 'a', 'girl', 'who', 'the', 'dude', 'has', 'a', 'crush', 'on', 'but', 'what', 's', 'this', 'the', 'trucker', 'is', 'still', 'on', 'their', 'tail', 'and', 'is', 'now', 'harassing', 'all', 'three', 'of', 'the', 'young', 'whippersnappers', 'you', 'bet', 'buckle', 'up', 'dorothy', 'this', 'is', 'gonna', 'be', 'one', 'bumpy', 'ride', 'critique', 'a', 'good', 'ol', 'time', 'at', 'the', 'movies', 'here', 's', 'a', 'film', 'that', 'actually', 'gives', 'away', 'most', 'of', 'its', 'plotline', 'in', 'its', 'trailer', 'and', 'doesn', 't', 'really', 'bring', 'anything', 'new', 'to', 'the', 'forefront', 'if', 'you', 've', 'seen', 'flicks', 'like', 'duel', 'and', 'breakdown', 'you', 've', 'crossed', 'this', 'path', 'before', 'but', 'still', 'manages', 'to', 'entertain', 'you', 'gangbusters', 'with', 'realistic', 'situations', 'believable', 'characters', 'funny', 'moments', 'thrills', 'chills', 'the', 'whole', 'shebang', 'let', 's', 'give', 'it', 'up', 'for', 'director', 'john', 'dahl', 'who', 'continues', 'to', 'put', 'out', 'solid', 'films', 'every', 'other', 'year', 'if', 'you', 'haven', 't', 'seen', 'red', 'rock', 'west', 'do', 'yourself', 'a', 'favor', 'right', 'now', 'and', 'jot', 'it', 'down', 'on', 'a', 'piece', 'of', 'paper', 'and', 'rent', 'it', 'at', 'your', 'earliest', 'convenience', 'and', 'much', 'like', 'that', 'film', 'this', 'one', 'has', 'an', 'excellent', 'premise', 'and', 'sets', 'everything', 'up', 'at', 'an', 'even', 'pace', 'it', 'gives', 'you', 'a', 'little', 'bit', 'of', 'background', 'on', 'each', 'of', 'the', 'main', 'three', 'characters', 'and', 'then', 'shows', 'you', 'how', 'one', 'small', 'prank', 'can', 'lead', 'to', 'a', 'whole', 'lotta', 'trouble', 'for', 'everyone', 'paul', 'walker', 'really', 'surprised', 'me', 'in', 'this', 'movie', 'since', 'i', 've', 'never', 'much', 'thought', 'of', 'him', 'as', 'anything', 'more', 'than', 'a', 'pretty', 'face', 'and', 'damn', 'is', 'it', 'ever', 'pretty', 'or', 'what', 'but', 'here', 'he', 'actually', 'manages', 'to', 'put', 'some', 'depth', 'behind', 'the', 'looks', 'and', 'that', 's', 'always', 'appreciated', 'in', 'films', 'in', 'which', 'you', 'are', 'so', 'closely', 'tied', 'to', 'the', 'main', 'characters', 'sobieski', 'is', 'also', 'good', 'but', 'she', 'isn', 't', 'in', 'the', 'movie', 'for', 'as', 'long', 'as', 'you', 'd', 'think', 'but', 'the', 'man', 'who', 'really', 'takes', 'this', 'film', 'to', 'another', 'level', 'is', 'steve', 'zahn', 'if', 'you', 've', 'loved', 'this', 'guy', 'as', 'the', 'goofball', 'in', 'most', 'of', 'his', 'previous', 'roles', 'you', 'll', 'appreciate', 'him', 'even', 'more', 'here', 'as', 'the', 'dude', 'who', 'starts', 'off', 'as', 'one', 'of', 'the', 'most', 'manic', 'and', 'excited', 'human', 'beings', 'i', 've', 'seen', 'in', 'quite', 'some', 'time', 'this', 'is', 'so', 'awesome', 'only', 'to', 'turn', 'into', 'a', 'man', 'scared', 'out', 'of', 'his', 'wits', 'by', 'the', 'end', 'of', 'the', 'flick', 'and', 'speaking', 'of', 'the', 'ending', 'boy', 'does', 'this', 'movie', 'deliver', 'some', 'chilling', 'moments', 'during', 'its', 'final', '15', 'clicks', 'or', 'what', 'the', 'arrow', 'and', 'i', 'were', 'practically', 'in', 'each', 'others', 'arms', 'well', 'maybe', 'i', 'm', 'exaggerating', 'but', 'you', 'catch', 'my', 'drift', 'as', 'each', 'minute', 'brought', 'about', 'another', 'turn', 'of', 'events', 'which', 'in', 'turn', 'took', 'it', 'all', 'to', 'an', 'even', 'higher', 'level', 'once', 'again', 'kudos', 'to', 'director', 'dahl', 'for', 'being', 'able', 'to', 'generate', 'that', 'type', 'of', 'intensity', 'suspense', 'and', 'tension', 'with', 'a', 'great', 'score', 'editing', 'style', 'and', 'camerawork', 'plot', 'wise', 'i', 'too', 'did', 'wonder', 'how', 'the', 'bad', 'guy', 'was', 'able', 'to', 'track', 'them', 'so', 'well', 'but', 'it', 'didn', 't', 'really', 'bother', 'me', 'all', 'that', 'much', 'you', 'can', 'assume', 'that', 'he', 'had', 'bugged', 'their', 'car', 'but', 'pretty', 'much', 'everything', 'else', 'in', 'the', 'story', 'stuck', 'like', 'glue', 'and', 'i', 'couldn', 't', 'help', 'but', 'put', 'myself', 'in', 'their', 'shoes', 'and', 'appreciate', 'their', 'thoroughly', 'desperate', 'circumstance', 'a', 'great', 'movie', 'with', 'an', 'even', 'cooler', 'ending', 'this', 'film', 'will', 'likely', 'be', 'remembered', 'as', 'one', 'of', 'the', 'better', 'thrillers', 'of', 'the', 'year', 'this', 'is', 'amazing', 'where', 's', 'joblo', 'coming', 'from', 'american', 'psycho', '10', '10', 'deep', 'blue', 'sea', '8', '10', 'eye', 'of', 'the', 'beholder', '4', '10', 'the', 'fast', 'and', 'the', 'furious', '7', '10', 'final', 'destination', '8', '10', 'the', 'glass', 'house', '6', '10', 'no', 'way', 'out', '8', '10']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'share', 'the', 'descent', 'into', 'darkness', 'of', 'a', 'talented', 'boy', 'pianist', 'years', 'later', 'we', 'see', 'his', 'subsequent', 'resurfacing', 'in', 'the', 'mid', '80', 's', 'a', 'damaged', 'man', 'walks', 'out', 'of', 'a', 'rainstorm', 'and', 'back', 'into', 'the', 'world', 'the', 'movie', 'charts', 'the', 'causes', 'of', 'his', 'mental', 'breakdown', 'based', 'on', 'the', 'life', 'story', 'of', 'david', 'helfgott', 'this', 'australian', 'film', 'is', 'a', 'rich', 'exploration', 'of', 'the', 'pressures', 'drilling', 'in', 'upon', 'a', 'child', 'genius', 'compounded', 'by', 'the', 'looming', 'shadow', 'of', 'a', 'domineering', 'father', 'in', 'the', '1950', 's', 'helfgott', 'emerged', 'as', 'a', 'child', 'prodigy', 'the', 'film', 'traces', 'his', 'relationship', 'with', 'his', 'father', 'whose', 'encouragement', 'comes', 'at', 'the', 'cost', 'of', 'demanding', 'david', 'have', 'no', 'life', 'beyond', 'that', 'of', 'the', 'black', 'and', 'white', 'keys', 'david', 'is', 'offered', 'places', 'in', 'the', 'usa', 'but', 'his', 'father', 'refuses', 'to', 'let', 'him', 'leave', 'australia', 'the', 'effect', 'this', 'has', 'upon', 'helfgott', 'is', 'cleverly', 'told', 'via', 'a', 'single', 'reverse', 'tracked', 'shot', 'david', 'is', 'sobbing', 'on', 'the', 'front', 'doorway', 'of', 'his', 'music', 'teacher', 's', 'house', 'he', 'is', 'well', 'lit', 'via', 'the', 'external', 'house', 'lights', 'his', 'teacher', 'isn', 't', 'home', 'we', 'pull', 'back', 'and', 'see', 'that', 'the', 'dark', 'front', 'room', 's', 'window', 'has', 'it', 's', 'curtains', 'open', 'there', 'in', 'a', 'pool', 'of', 'blackness', 'is', 'a', 'grand', 'piano', 'david', 's', 'fall', 'over', 'the', 'dark', 'cliff', 'has', 'begun', 'instead', 'of', 'being', 'told', 'in', 'a', 'linear', 'mode', 'shine', 'jumps', 'around', 'in', 'time', 'ala', 'pulp', 'fiction', 'this', 'narrative', 'device', 'works', 'well', 'we', 'see', 'david', 'as', 'a', 'boy', 'teenager', 'in', 'london', 'and', 'mental', 'patient', 'in', 'various', 'snippets', 'the', 'film', 'then', 'neatly', 'loops', 'back', 'for', 'its', 'uplifting', 'conclusion', 'adult', 'helfgott', 's', 'return', 'to', 'the', 'light', 'is', 'told', 'in', 'linear', 'fashion', 'the', 'lead', 'role', 'is', 'played', 'by', '3', 'different', 'actors', 'with', 'the', 'teenage', 'david', 'taylor', 'and', 'adult', 'david', 'rush', 'standing', 'out', 'whilst', 'rush', 'is', 'known', 'for', 'his', 'tv', 'work', 'in', 'australia', 'i', 'think', 'this', 'is', 'his', 'first', 'major', 'cinema', 'work', 'his', 'david', 'whilst', 'clearly', 'damaged', 'he', 'keeps', 'quoting', 'his', 'daddy', 'in', 'a', 'babbling', 'word', 'torrent', 'cuddles', 'strangers', 'and', 'grabs', 'women', 'on', 'the', 'breasts', 'is', 'both', 'touching', 'and', 'funny', 'taylor', 's', 'teenage', 'david', 'is', 'the', 'more', 'sympathy', 'producing', 'role', 'we', 'are', 'mute', 'witness', 'to', 'his', 'gradual', 'breakdown', 'as', 'the', 'pressures', 'just', 'keep', 'on', 'building', 'his', 'father', 'wants', 'him', 'to', 'play', 'the', 'very', 'demanding', 'rachmaninov', 's', '3rd', 'whilst', 'his', 'teacher', 'wants', 'to', 'start', 'him', 'with', 'a', 'simple', 'mozart', 'technically', 'this', 'is', 'a', 'beautiful', 'film', 'and', 'a', 'lot', 'of', 'thought', 'has', 'gone', 'in', 'to', 'the', 'production', 'i', 'particularly', 'like', 'the', 'use', 'of', 'slow', 'motion', 'and', 'sound', 'effects', 'one', 'scene', 'has', 'adult', 'helfgott', 'happily', 'babbling', 'in', 'a', 'car', 'in', 'the', 'rain', 'the', 'sound', 'of', 'the', 'windshield', 'wipers', 'fades', 'up', 'over', 'his', 'voice', 'with', 'a', 'clever', 'subtlety', 'the', 'wiper', 'sound', 'appears', 'to', 'morph', 'into', 'a', 'rhythmic', 'thumping', 'but', 'of', 'the', 'same', 'beat', 'the', 'image', 'dissolves', 'and', 'we', 'realise', 'the', 'thumping', 'is', 'the', 'slow', 'motion', 'applause', 'of', 'a', 'crowd', 'but', 'it', 's', 'us', 'being', 'clapped', 'we', 'are', 'young', 'david', 'walking', 'up', 'to', 'the', 'stage', 'for', 'our', 'first', 'public', 'show', 'the', 'music', 'as', 'you', 'would', 'expect', 'from', 'a', 'film', 'of', 'this', 'subject', 'matter', 'is', 'critical', 'and', 'it', 'works', 'very', 'well', 'the', 'miming', 'of', 'the', 'actors', 'piano', 'playing', 'is', 'faultless', 'apparently', 'rush', 'didn', 't', 'mime', 'he', 'really', 'did', 'play', 'helfgott', 's', 'life', 'is', 'certainly', 'not', 'without', 'it', 's', 'sympathetic', 'people', 'indeed', 'his', 'return', 'to', 'the', 'normal', 'world', 'is', 'accelerated', 'by', 'his', 'chance', 'meetings', 'with', 'two', 'very', 'special', 'women', 'his', 'ultimately', 'uplifting', 'story', 'is', 'a', 'magic', 'piece', 'of', 'theatre', 'craft', 'it', 'shows', 'that', 'with', 'the', 'help', 'of', 'god', 'we', 'can', 'all', 'find', 'our', 'way', 'back', 'home']\n",
      "1\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-08f152166ace>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mnumberOfRuntimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumberOfRuntimes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mNBscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLRscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowerCase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlaplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposPrior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0maccuracyNB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNBscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mf1NB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNBscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c2a16666d041>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(dataset, stemming, lowerCase, laplace, posPrior)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpredictedLabelsNB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlaplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposPrior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mpredictedLabelsLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mpredictedLabelsSVM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msupportVectorMachine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mpredictedLabelsDT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-d7f2296cfd7a>\u001b[0m in \u001b[0;36mlogisticRegression\u001b[1;34m(train_set, train_labels, dev_set)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mLRclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = \"../TermProject/txt_sentoken\"\n",
    "    stemming = False\n",
    "    lowerCase = True\n",
    "    laplace = 1.0\n",
    "    posPrior = 0.8\n",
    "    ##Naive bayes\n",
    "    accuracyNB = []\n",
    "    f1NB = []\n",
    "    precisionNB = []\n",
    "    recallNB = []\n",
    "    #Logistic regression\n",
    "    accuracyLR = []\n",
    "    f1LR = []\n",
    "    precisionLR = []\n",
    "    recallLR = []\n",
    "    #Support Vector Machine\n",
    "    accuracySVM = []\n",
    "    f1SVM = []\n",
    "    precisionSVM = []\n",
    "    recallSVM = []\n",
    "    #Decision Tree\n",
    "    accuracyDT = []\n",
    "    f1DT = []\n",
    "    precisionDT = []\n",
    "    recallDT = []\n",
    "    numberOfRuntimes = 5\n",
    "    for i in range(numberOfRuntimes):\n",
    "        NBscores, LRscores, SVMscores = main(dataset, stemming, lowerCase, laplace, posPrior)\n",
    "        accuracyNB.append(NBscores[0])\n",
    "        f1NB.append(NBscores[1])\n",
    "        precisionNB.append(NBscores[2])\n",
    "        recallNB.append(NBscores[3])\n",
    "        \n",
    "        accuracyLR.append(LRscores[0])\n",
    "        f1LR.append(LRscores[1])\n",
    "        precisionLR.append(LRscores[2])\n",
    "        recallLR.append(LRscores[3])\n",
    "        \n",
    "        accuracySVM.append(SVMscores[0])\n",
    "        f1SVM.append(SVMscores[1])\n",
    "        precisionSVM.append(SVMscores[2])\n",
    "        recallSVM.append(SVMscores[3])\n",
    "        \n",
    "        accuracyDT.append(DTscores[0])\n",
    "        f1DT.append(DTscores[1])\n",
    "        precisionDT.append(DTscores[2])\n",
    "        recallDT.append(DTscores[3])\n",
    "        \n",
    "#         print(\"RUN NUMBER \" + str(i+1) + \" ---------------\")\n",
    "#         print(\"Accuracy:\",curaccuracy)\n",
    "#         print(\"F1-Score:\",curf1)\n",
    "#         print(\"Precision:\",curprecision)\n",
    "#         print(\"Recall:\",currecall)\n",
    "\n",
    "    #RESULTS OF NAIVE BAYES (unigram) \n",
    "    aveAccuracy = np.mean(accuracyNB)\n",
    "    avef1 = np.mean(f1NB)\n",
    "    avePrecision = np.mean(precisionNB)\n",
    "    aveRecall = np.mean(recallNB)\n",
    "    stdAccuracy = np.std(accuracyNB)\n",
    "    stdf1 = np.std(f1NB)\n",
    "    stdPrecision = np.std(precisionNB)\n",
    "    stdRecall = np.std(recallNB)\n",
    "    print(\"Final results NAIVE BAYES----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF LOGISTIC REGRESSION\n",
    "    aveAccuracy = np.mean(accuracyLR)\n",
    "    avef1 = np.mean(f1LR)\n",
    "    avePrecision = np.mean(precisionLR)\n",
    "    aveRecall = np.mean(recallLR)\n",
    "    stdAccuracy = np.std(accuracyLR)\n",
    "    stdf1 = np.std(f1LR)\n",
    "    stdPrecision = np.std(precisionLR)\n",
    "    stdRecall = np.std(recallLR)\n",
    "    print(\"Final results  LOGISTIC REGRESSION----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF SUPPORT VECTOR MACHINE\n",
    "    aveAccuracy = np.mean(accuracySVM)\n",
    "    avef1 = np.mean(f1SVM)\n",
    "    avePrecision = np.mean(precisionSVM)\n",
    "    aveRecall = np.mean(recallSVM)\n",
    "    stdAccuracy = np.std(accuracySVM)\n",
    "    stdf1 = np.std(f1SVM)\n",
    "    stdPrecision = np.std(precisionSVM)\n",
    "    stdRecall = np.std(recallSVM)\n",
    "    print(\"Final results SUPPORT VECTOR MACHINE----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF DECISION TREE\n",
    "    aveAccuracy = np.mean(accuracyDT)\n",
    "    avef1 = np.mean(f1DT)\n",
    "    avePrecision = np.mean(precisionDT)\n",
    "    aveRecall = np.mean(recallDT)\n",
    "    stdAccuracy = np.std(accuracyDT)\n",
    "    stdf1 = np.std(f1DT)\n",
    "    stdPrecision = np.std(precisionDT)\n",
    "    stdRecall = np.std(recallDT)\n",
    "    print(\"Final results DECISION TREE----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
