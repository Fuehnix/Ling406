{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import string\n",
    "import glob\n",
    "import numpy as np\n",
    "import nltk.sentiment.util\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.metrics import *\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadReviewNames(posDir, negDir):\n",
    "    positive = glob.glob(posDir)\n",
    "    negative = glob.glob(negDir)\n",
    "#     print(positive)\n",
    "#     print(negative)\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads dataset in a way that will work with scikit\n",
    "def getPosNegReviews(directory):\n",
    "    positive, negative = loadReviewNames(directory + '/pos/*',directory + '/neg/*')\n",
    "    random.shuffle(positive)\n",
    "    boundaryTrain = math.floor(0.8 * len(positive))\n",
    "    trainPos = positive[:boundaryTrain]\n",
    "    random.shuffle(negative)\n",
    "    trainNeg = negative[:boundaryTrain]\n",
    "    \n",
    "    testPos = positive[boundaryTrain:]\n",
    "    testNeg = negative[boundaryTrain:]\n",
    "    return trainPos,trainNeg,testPos,testNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDir(name):\n",
    "    # Loads the files in the folder and returns a list of lists of words from the text in each file\n",
    "    if Stemming:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = []\n",
    "    count = 0\n",
    "    for f in tqdm(listdir(name)):\n",
    "        fullname = name+f\n",
    "        text = []\n",
    "        with open(fullname, 'rb') as f:\n",
    "            for line in f:\n",
    "                if Lowercase:\n",
    "                    line = line.decode(errors='ignore').lower()\n",
    "                    text += tokenizer.tokenize(line)\n",
    "                else:\n",
    "                    text += tokenizer.tokenize(line.decode(errors='ignore'))\n",
    "        if Stemming:\n",
    "            for i in range(len(text)):\n",
    "#                 if text[i] in bad_words:\n",
    "#                     continue\n",
    "                text[i] = porter_stemmer.stem(text[i])\n",
    "        data.append(text)\n",
    "        count = count + 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads dataset in a way that will work with my unigram Naive bayes implementation\n",
    "def loadDatasetNB(directory):\n",
    "    positive = loadDir(directory + '/pos/')\n",
    "    negative = loadDir(directory + '/neg/')\n",
    "    random.shuffle(positive)\n",
    "    boundaryTrain = math.floor(0.8 * len(positive))\n",
    "    trainPos = positive[:boundaryTrain]\n",
    "    random.shuffle(negative)\n",
    "    trainNeg = negative[:boundaryTrain]\n",
    "    combinedTrain = trainPos + trainNeg\n",
    "    length = len(trainPos) + len(trainNeg)\n",
    "    labelsTrain = len(trainNeg) * [1] + len(trainNeg) * [0]\n",
    "    labelsTrain = np.array(labelsTrain)\n",
    "\n",
    "    testPos = positive[boundaryTrain:]\n",
    "    testNeg = negative[boundaryTrain:]\n",
    "    combinedTest = testPos + testNeg\n",
    "    labelsTest = len(testPos) * [1] + len(testNeg) * [0]\n",
    "    labelsTest = np.array(labelsTest)\n",
    "    return combinedTrain, labelsTrain, combinedTest, labelsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for unigram Naive bayes implementation\n",
    "def bagOfWordsNB(train_set, train_labels):\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    mydict = {}\n",
    "    smoothing_parameter = 0.034\n",
    "    posV = 0\n",
    "    negV = 0\n",
    "    totalposwords = 0\n",
    "    totalnegwords = 0\n",
    "\n",
    "#     start = time.process_time()\n",
    "\n",
    "    #create bag of words and number of occurences\n",
    "    count = 0\n",
    "    if Stop:\n",
    "        for x in train_set:\n",
    "            rating = train_labels[count]\n",
    "            count += 1\n",
    "            if(rating):\n",
    "                for y in x:\n",
    "                    if y not in mydict and y not in stops:\n",
    "                        mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "                        posV += 1\n",
    "                        totalposwords += 1\n",
    "                    elif y not in stops:\n",
    "                        if mydict[y][0] == 0:\n",
    "                            posV += 1\n",
    "                        mydict[y][0] += 1\n",
    "                        totalposwords += 1\n",
    "            else:\n",
    "                for y in x:\n",
    "                    if y not in mydict and y not in stops:\n",
    "                        mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "                        negV += 1\n",
    "                        totalnegwords += 1\n",
    "                    elif y not in stops:\n",
    "                        if mydict[y][1] == 0:\n",
    "                            negV += 1\n",
    "                        mydict[y][1] += 1\n",
    "                        totalnegwords += 1\n",
    "    else:\n",
    "        for x in train_set:\n",
    "            rating = train_labels[count]\n",
    "            count += 1\n",
    "            if(rating):\n",
    "                for y in x:\n",
    "                    if y not in mydict:\n",
    "                        mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "                        posV += 1\n",
    "                        totalposwords += 1\n",
    "                    else:\n",
    "                        if mydict[y][0] == 0:\n",
    "                            posV += 1\n",
    "                        mydict[y][0] += 1\n",
    "                        totalposwords += 1\n",
    "            else:\n",
    "                for y in x:\n",
    "                    if y not in mydict:\n",
    "                        mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "                        negV += 1\n",
    "                        totalnegwords += 1\n",
    "                    else:\n",
    "                        if mydict[y][1] == 0:\n",
    "                            negV += 1\n",
    "                        mydict[y][1] += 1\n",
    "                        totalnegwords += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "    BOW = mydict, posV, negV, totalposwords, totalnegwords\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes(train_set, train_labels, dev_set):\n",
    "    #Baseline#\n",
    "    # return predicted labels of development set\n",
    "    # print(\"not even started yet\")\n",
    "    smoothing_parameter = 0.034\n",
    "\n",
    "#     start = time.process_time()\n",
    "\n",
    "#     print(\"Going through train set took: \", time.process_time() - start)\n",
    "    mydict, posV, negV, totalposwords, totalnegwords = bagOfWordsNB(train_set, train_labels)\n",
    "    \n",
    "    \n",
    "    #come up with the bag of words unigram model\n",
    "    probWordPos = {}\n",
    "    probWordNeg = {}\n",
    "\n",
    "    for x in mydict:\n",
    "        #use laplace smoothing\n",
    "        # count(W) + a / n + a * (V+1)\n",
    "        # n = number of words in our training data\n",
    "        # count(W) = number of times W appeared in training data\n",
    "        # α is a tuning constant between 0 and 1 (typically small)\n",
    "        # V = number of word TYPES seen in training data\n",
    "        if(Smoothing):\n",
    "            probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "            probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        else:\n",
    "            probWordPos[x] = math.log((mydict[x][0]) / (totalposwords))\n",
    "            probWordNeg[x] = math.log((mydict[x][1]) / (totalnegwords))\n",
    "#     start = time.process_time()\n",
    "\n",
    "    # dev set\n",
    "    predictions = []\n",
    "    for x in range(len(dev_set)):\n",
    "        chancePos = 0\n",
    "        chanceNeg = 0 \n",
    "        if(Prior):\n",
    "            chancePos += math.log(posPrior)\n",
    "            chanceNeg += math.log(1-posPrior)\n",
    "        for y in range(len(dev_set[x])):\n",
    "            if dev_set[x][y] in mydict:\n",
    "                chancePos += probWordPos[dev_set[x][y]]\n",
    "                chanceNeg += probWordNeg[dev_set[x][y]]\n",
    "            # else:\n",
    "                # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "                # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        if(chancePos > chanceNeg):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuraciesNB(predictedLabels, dev_set, dev_labels):\n",
    "    yhats = predictedLabels\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    tp = np.sum([yhats[i] == dev_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    precision = tp / np.sum([yhats[i] == 1 for i in range(len(yhats))])\n",
    "    recall = tp / (np.sum([yhats[i] != dev_labels[i] and yhats[i] == 0 for i in range(len(yhats))]) + tp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuraciesSK(predictedLabels):\n",
    "    dev_labels = [1]*200+[0]*200\n",
    "    yhats = predictedLabels\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    tp = np.sum([yhats[i] == dev_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    precision = tp / np.sum([yhats[i] == 1 for i in range(len(yhats))])\n",
    "    recall = tp / (np.sum([yhats[i] != dev_labels[i] and yhats[i] == 0 for i in range(len(yhats))]) + tp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scikit pre processing methods ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWordsSK(reviews):\n",
    "    word_bag = {}\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    for review in reviews:\n",
    "        with open(review, 'r') as f:\n",
    "            line = f.read()\n",
    "            line = line.split(\" \")\n",
    "            if Stop:\n",
    "                line = [word for word in line if word not in set(stops)]\n",
    "            if Alpha:\n",
    "                line = [word for word in line if word.isalpha()]\n",
    "            for word in line:\n",
    "                word_bag[word] = word_bag.get(word, 0) + 1\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimBags(pWbag, nWbag):\n",
    "    posList = {}\n",
    "    negList = {}\n",
    "    print(\"prefilter\")\n",
    "    print(len(pWbag))\n",
    "    print(len(nWbag))\n",
    "    #force words to only exist in either positive or negative bags\n",
    "    if(Unique):\n",
    "        for key in nWbag.keys():\n",
    "            if key in pWbag.keys():\n",
    "                posFreq = pWbag[key]\n",
    "                negFreq = nWbag[key]\n",
    "                if int(posFreq) >= int(negFreq):\n",
    "                    posList[key] = pWbag[key]\n",
    "                elif int(posFreq) < int(negFreq):\n",
    "                    negList[key] = nWbag[key]\n",
    "            else:\n",
    "                negList[key] = nWbag[key]\n",
    "        for key in pWbag.keys():\n",
    "            if key not in nWbag.keys():\n",
    "                posList[key] = pWbag[key]\n",
    "    else:\n",
    "        posList = pWbag\n",
    "        negList = nWbag\n",
    "    print(\"first filter\")\n",
    "    print(len(posList))\n",
    "    print(len(negList))\n",
    "    #determine the boundary size\n",
    "    max_length=min(len(negList),len(posList))\n",
    "    print(\"max_length\")\n",
    "    print(max_length)\n",
    "    #sort the keys of the map into a list, sorting to get most frequently used words\n",
    "    sortedNegList = sorted(negList, key=negList.get, reverse=True)[:max_length]\n",
    "    sortedPosList = sorted(posList, key=posList.get, reverse=True)[:max_length]\n",
    "    print(\"sorted max filter\")\n",
    "    print(len(sortedNegList))\n",
    "    print(len(sortedPosList))\n",
    "    #turn the sorted list into a usable mapping again\n",
    "    new_neg= {k:negList[k] for k in sortedNegList}\n",
    "    new_pos = {k:posList[k] for k in sortedPosList}\n",
    "#     print(\"k filter\")\n",
    "#     print(len(new_pos))\n",
    "#     print(len(new_neg))\n",
    "    return new_pos,new_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenize(line, stop=True, negation=True):\n",
    "    line = line.split(\" \")\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    if Stop:\n",
    "        line = [word for word in line if word not in set(stops)]\n",
    "    if Alpha:\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "    if Negation:\n",
    "        line = nltk.sentiment.util.mark_negation(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSortedKeyList(bag1,bag2):\n",
    "    keyList = [*bag1] + list(set([*bag2]) - set([*bag1]))\n",
    "    keyList = sorted(keyList)\n",
    "    return keyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skPreprocessing(dataset):\n",
    "    trainPos,trainNeg,testPos,testNeg = getPosNegReviews(dataset)\n",
    "    pWbag = bagOfWordsSK(trainPos)\n",
    "    nWbag = bagOfWordsSK(trainNeg)\n",
    "    pos_keys, neg_keys = trimBags(pWbag, nWbag)\n",
    "    keyList = getSortedKeyList(pWbag,nWbag)\n",
    "    cv = CountVectorizer(input='filename', tokenizer=myTokenize, lowercase=True, vocabulary=keyList)\n",
    "    trainFileNames = trainPos + trainNeg\n",
    "    testFileNames = testPos + testNeg\n",
    "    trainSet = cv.fit_transform(trainFileNames)\n",
    "    testSet = cv.fit_transform(testFileNames)\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(trainSet, testSet):\n",
    "    LRclassifier = LogisticRegression()\n",
    "    labels = [1]*800+[0]*800\n",
    "    LRclassifier.fit(trainSet,labels)\n",
    "    predictions = LRclassifier.predict(testSet)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportVectorMachine(trainSet, testSet):\n",
    "    SVMclassifier = LinearSVC()\n",
    "    labels = [1]*800+[0]*800\n",
    "    SVMclassifier.fit(trainSet,labels)\n",
    "    predictions = SVMclassifier.predict(testSet)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree(trainSet, testSet):\n",
    "    SVMclassifier = DecisionTreeClassifier()\n",
    "    labels = [1]*800+[0]*800\n",
    "    SVMclassifier.fit(trainSet,labels)\n",
    "    predictions = SVMclassifier.predict(testSet)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, stemming, lowerCase,stop, negation, laplace, posPrior):\n",
    "    trainSet, trainLabels, revSet, revLabels = loadDatasetNB(dataset)\n",
    "    trainSetSK, testSetSK = skPreprocessing(dataset)\n",
    "\n",
    "    predictedLabelsNB = naiveBayes(trainSet, trainLabels, revSet)\n",
    "    predictedLabelsLR = logisticRegression(trainSetSK, testSetSK)\n",
    "    predictedLabelsSVM = supportVectorMachine(trainSetSK, testSetSK)\n",
    "    predictedLabelsDT = decisionTree(trainSetSK,testSetSK)\n",
    "    \n",
    "    accuracyNB, f1NB, precisionNB, recallNB = compute_accuraciesNB(predictedLabelsNB, revSet, revLabels)\n",
    "    accuracyLR, f1LR, precisionLR, recallLR = compute_accuraciesSK(predictedLabelsLR)\n",
    "    accuracySVM, f1SVM, precisionSVM, recallSVM = compute_accuraciesSK(predictedLabelsSVM)\n",
    "    accuracyDT, f1DT, precisionDT, recallDT = compute_accuraciesSK(predictedLabelsDT)\n",
    "    \n",
    "    NBscores = accuracyNB, f1NB, precisionNB, recallNB\n",
    "    LRscores = accuracyLR, f1LR, precisionLR, recallLR\n",
    "    SVMscores = accuracySVM, f1SVM, precisionSVM, recallSVM\n",
    "    DTscores = accuracyDT, f1DT, precisionDT, recallDT\n",
    "#     print(\"Accuracy:\",accuracy)\n",
    "#     print(\"F1-Score:\",f1)\n",
    "#     print(\"Precision:\",precision)\n",
    "#     print(\"Recall:\",recall)\n",
    "    return NBscores, LRscores, SVMscores, DTscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2881.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3213.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefilter\n",
      "26026\n",
      "24339\n",
      "first filter\n",
      "20379\n",
      "13825\n",
      "max_length\n",
      "13825\n",
      "sorted max filter\n",
      "13825\n",
      "13825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2506.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3029.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefilter\n",
      "26180\n",
      "24284\n",
      "first filter\n",
      "20644\n",
      "13654\n",
      "max_length\n",
      "13654\n",
      "sorted max filter\n",
      "13654\n",
      "13654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2645.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2156.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefilter\n",
      "25822\n",
      "24492\n",
      "first filter\n",
      "20279\n",
      "13826\n",
      "max_length\n",
      "13826\n",
      "sorted max filter\n",
      "13826\n",
      "13826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results NAIVE BAYES----------------------------------\n",
      "Average Accuracy: 0.7616666666666667\n",
      "Average F1: 0.7625950638581905\n",
      "Average Precision: 0.7593664457699546\n",
      "Average recall 0.7666666666666666\n",
      "STD Accuracy: 0.02044640691064217\n",
      "STD F1: 0.022316368582995467\n",
      "STD Precision: 0.019887520316892126\n",
      "STD Recall: 0.03472111109333279\n",
      "Final results  LOGISTIC REGRESSION----------------------------------\n",
      "Average Accuracy: 0.7341666666666667\n",
      "Average F1: 0.735320295023977\n",
      "Average Precision: 0.7325496405986812\n",
      "Average recall 0.7383333333333333\n",
      "STD Accuracy: 0.011606990230986759\n",
      "STD F1: 0.009913399280773644\n",
      "STD Precision: 0.015947254089400356\n",
      "STD Recall: 0.010274023338281637\n",
      "Final results SUPPORT VECTOR MACHINE----------------------------------\n",
      "Average Accuracy: 0.6616666666666666\n",
      "Average F1: 0.6650307973323138\n",
      "Average Precision: 0.6588363570591612\n",
      "Average recall 0.6716666666666667\n",
      "STD Accuracy: 0.007168604389202217\n",
      "STD F1: 0.0035884540390672404\n",
      "STD Precision: 0.011393418836333226\n",
      "STD Recall: 0.010274023338281637\n",
      "Final results DECISION TREE----------------------------------\n",
      "Average Accuracy: 0.5991666666666666\n",
      "Average F1: 0.597316591926283\n",
      "Average Precision: 0.5998059927261382\n",
      "Average recall 0.5950000000000001\n",
      "STD Accuracy: 0.012304019216861174\n",
      "STD F1: 0.01650838623623474\n",
      "STD Precision: 0.010523452895359624\n",
      "STD Recall: 0.02273030282830978\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = \"../TermProject/txt_sentoken\"\n",
    "    dataset = \"../TermProject/txt_sentoken\"\n",
    "    Stemming = False\n",
    "    Lowercase = True\n",
    "    Stop = True\n",
    "    Alpha = True\n",
    "    Negation = True\n",
    "    Unique = True\n",
    "    Smoothing = True\n",
    "    Prior = True\n",
    "    laplace = 0.034\n",
    "    posPrior = 0.8\n",
    "    \n",
    "    ##Naive bayes\n",
    "    accuracyNB = []\n",
    "    f1NB = []\n",
    "    precisionNB = []\n",
    "    recallNB = []\n",
    "    \n",
    "    #Logistic regression\n",
    "    accuracyLR = []\n",
    "    f1LR = []\n",
    "    precisionLR = []\n",
    "    recallLR = []\n",
    "    \n",
    "    #Support Vector Machine\n",
    "    accuracySVM = []\n",
    "    f1SVM = []\n",
    "    precisionSVM = []\n",
    "    recallSVM = []\n",
    "    \n",
    "    #Decision Tree\n",
    "    accuracyDT = []\n",
    "    f1DT = []\n",
    "    precisionDT = []\n",
    "    recallDT = []\n",
    "    \n",
    "    numberOfRuntimes = 3\n",
    "    for i in range(numberOfRuntimes):\n",
    "        NBscores, LRscores, SVMscores, DTscores = main(dataset, stemming, lowerCase,stop, negation, laplace, posPrior)\n",
    "        accuracyNB.append(NBscores[0])\n",
    "        f1NB.append(NBscores[1])\n",
    "        precisionNB.append(NBscores[2])\n",
    "        recallNB.append(NBscores[3])\n",
    "        \n",
    "        accuracyLR.append(LRscores[0])\n",
    "        f1LR.append(LRscores[1])\n",
    "        precisionLR.append(LRscores[2])\n",
    "        recallLR.append(LRscores[3])\n",
    "        \n",
    "        accuracySVM.append(SVMscores[0])\n",
    "        f1SVM.append(SVMscores[1])\n",
    "        precisionSVM.append(SVMscores[2])\n",
    "        recallSVM.append(SVMscores[3])\n",
    "        \n",
    "        accuracyDT.append(DTscores[0])\n",
    "        f1DT.append(DTscores[1])\n",
    "        precisionDT.append(DTscores[2])\n",
    "        recallDT.append(DTscores[3])\n",
    "        \n",
    "#         print(\"RUN NUMBER \" + str(i+1) + \" ---------------\")\n",
    "#         print(\"Accuracy:\",curaccuracy)\n",
    "#         print(\"F1-Score:\",curf1)\n",
    "#         print(\"Precision:\",curprecision)\n",
    "#         print(\"Recall:\",currecall)\n",
    "\n",
    "    #RESULTS OF NAIVE BAYES (unigram) \n",
    "    aveAccuracy = np.mean(accuracyNB)\n",
    "    avef1 = np.mean(f1NB)\n",
    "    avePrecision = np.mean(precisionNB)\n",
    "    aveRecall = np.mean(recallNB)\n",
    "    stdAccuracy = np.std(accuracyNB)\n",
    "    stdf1 = np.std(f1NB)\n",
    "    stdPrecision = np.std(precisionNB)\n",
    "    stdRecall = np.std(recallNB)\n",
    "    print(\"Final results NAIVE BAYES----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF LOGISTIC REGRESSION\n",
    "    aveAccuracy = np.mean(accuracyLR)\n",
    "    avef1 = np.mean(f1LR)\n",
    "    avePrecision = np.mean(precisionLR)\n",
    "    aveRecall = np.mean(recallLR)\n",
    "    stdAccuracy = np.std(accuracyLR)\n",
    "    stdf1 = np.std(f1LR)\n",
    "    stdPrecision = np.std(precisionLR)\n",
    "    stdRecall = np.std(recallLR)\n",
    "    print(\"Final results  LOGISTIC REGRESSION----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF SUPPORT VECTOR MACHINE\n",
    "    aveAccuracy = np.mean(accuracySVM)\n",
    "    avef1 = np.mean(f1SVM)\n",
    "    avePrecision = np.mean(precisionSVM)\n",
    "    aveRecall = np.mean(recallSVM)\n",
    "    stdAccuracy = np.std(accuracySVM)\n",
    "    stdf1 = np.std(f1SVM)\n",
    "    stdPrecision = np.std(precisionSVM)\n",
    "    stdRecall = np.std(recallSVM)\n",
    "    print(\"Final results SUPPORT VECTOR MACHINE----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF DECISION TREE\n",
    "    aveAccuracy = np.mean(accuracyDT)\n",
    "    avef1 = np.mean(f1DT)\n",
    "    avePrecision = np.mean(precisionDT)\n",
    "    aveRecall = np.mean(recallDT)\n",
    "    stdAccuracy = np.std(accuracyDT)\n",
    "    stdf1 = np.std(f1DT)\n",
    "    stdPrecision = np.std(precisionDT)\n",
    "    stdRecall = np.std(recallDT)\n",
    "    print(\"Final results DECISION TREE----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
