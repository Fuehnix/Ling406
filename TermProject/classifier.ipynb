{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# bad_words = {'aed','oed','eed'} # these words fail in nltk stemmer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDir(name,stemming,lower_case):\n",
    "    # Loads the files in the folder and returns a list of lists of words from the text in each file\n",
    "    if stemming:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    X0 = []\n",
    "    count = 0\n",
    "    for f in tqdm(listdir(name)):\n",
    "        fullname = name+f\n",
    "        text = []\n",
    "        with open(fullname, 'rb') as f:\n",
    "            for line in f:\n",
    "                if lower_case:\n",
    "                    line = line.decode(errors='ignore').lower()\n",
    "                    text += tokenizer.tokenize(line)\n",
    "                else:\n",
    "                    text += tokenizer.tokenize(line.decode(errors='ignore'))\n",
    "        if stemming:\n",
    "            for i in range(len(text)):\n",
    "#                 if text[i] in bad_words:\n",
    "#                     continue\n",
    "                text[i] = porter_stemmer.stem(text[i])\n",
    "        X0.append(text)\n",
    "        count = count + 1\n",
    "    return X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(directory, stemming, lower_case):\n",
    "    positive = loadDir(directory + '/pos/',stemming, lower_case)\n",
    "    negative = loadDir(directory + '/neg/',stemming, lower_case)\n",
    "    random.shuffle(positive)\n",
    "    boundaryTrain = math.floor(0.8 * len(positive))\n",
    "    trainPos = positive[:boundaryTrain]\n",
    "    random.shuffle(negative)\n",
    "    trainNeg = negative[:boundaryTrain]\n",
    "    combinedTrain = trainPos + trainNeg\n",
    "    length = len(trainPos) + len(trainNeg)\n",
    "    labelsTrain = len(trainNeg) * [1] + len(trainNeg) * [0]\n",
    "    labelsTrain = np.array(labelsTrain)\n",
    "\n",
    "    testPos = positive[boundaryTrain:]\n",
    "    testNeg = negative[boundaryTrain:]\n",
    "    combinedTest = testPos + testNeg\n",
    "    labelsTest = len(testPos) * [1] + len(testNeg) * [0]\n",
    "    labelsTest = np.array(labelsTest)\n",
    "    return combinedTrain, labelsTrain, combinedTest, labelsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import math\n",
    "\n",
    "STEMMING = True\n",
    "LOWER_CASE = True\n",
    "\n",
    "def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "    \"\"\"\n",
    "    train_set - List of list of words corresponding with each movie review\n",
    "    example: suppose I had two reviews 'like this movie' and 'i fall asleep' in my training set\n",
    "    Then train_set := [['like','this','movie'], ['i','fall','asleep']]\n",
    "\n",
    "    train_labels - List of labels corresponding with train_set\n",
    "    example: Suppose I had two reviews, first one was positive and second one was negative.\n",
    "    Then train_labels := [1, 0]\n",
    "\n",
    "    dev_set - List of list of words corresponding with each review that we are testing on\n",
    "              It follows the same format as train_set\n",
    "\n",
    "    smoothing_parameter - The smoothing parameter you provided with --laplace (1.0 by default)\n",
    "    \"\"\"\n",
    "    # TODO: Write your code here\n",
    "    # return predicted labels of development set\n",
    "    # print(\"not even started yet\")\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    mydict = {}\n",
    "    smoothing_parameter = 0.034\n",
    "    posV = 0\n",
    "    negV = 0\n",
    "    totalposwords = 0\n",
    "    totalnegwords = 0\n",
    "\n",
    "    start = time.process_time()\n",
    "#     print(\"creating occurences and wordlist\")\n",
    "\n",
    "    #create bag of words and number of occurences\n",
    "    count = 0\n",
    "    for x in train_set:\n",
    "        rating = train_labels[count]\n",
    "        count += 1\n",
    "        if(rating):\n",
    "            for y in x:\n",
    "                if y not in mydict and y not in stops:\n",
    "                    mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "                    posV += 1\n",
    "                    totalposwords += 1\n",
    "                elif y not in stops:\n",
    "                    if mydict[y][0] == 0:\n",
    "                        posV += 1\n",
    "                    mydict[y][0] += 1\n",
    "                    totalposwords += 1\n",
    "        else:\n",
    "            for y in x:\n",
    "                if y not in mydict and y not in stops:\n",
    "                    mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "                    negV += 1\n",
    "                    totalnegwords += 1\n",
    "                elif y not in stops:\n",
    "                    if mydict[y][1] == 0:\n",
    "                        negV += 1\n",
    "                    mydict[y][1] += 1\n",
    "                    totalnegwords += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "#     print(\"Going through train set took: \", time.process_time() - start)\n",
    "\n",
    "    \n",
    "    #come up with the bag of words unigram model\n",
    "    probWordPos = {}\n",
    "    probWordNeg = {}\n",
    "    # PosII = 0\n",
    "    # NegII = 0\n",
    "    start = time.process_time()\n",
    "#     print(\"calculate prob\")\n",
    "    for x in mydict:\n",
    "        #use laplace smoothing\n",
    "        # count(W) + a / n + a * (V+1)\n",
    "        # n = number of words in our UK training data\n",
    "        # count(W) = number of times W appeared in UK training data\n",
    "        # α is a tuning constant between 0 and 1 (typically small)\n",
    "        # V = number of word TYPES seen in training data\n",
    "\n",
    "        probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "        probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        #calculate that II symbol that is basically summation, but using mutiplication\n",
    "        #logs are used because we are working with incredibly small numbers\n",
    "        # PosII += (probWordPos[x])\n",
    "        # NegII += (probWordNeg[x])\n",
    "    \n",
    "\n",
    "    #unneccessary calculations on prob of train set\n",
    "#     print(\"Prob calculations: \", time.process_time() - start)\n",
    "    # print(\"PosII\", PosII)\n",
    "    # print(\"NegII\", NegII)\n",
    "    # probPos = math.log(pos_prior) + PosII\n",
    "    # probNeg = math.log(1 - pos_prior) + NegII\n",
    "\n",
    "    # print(\"positive\", probPos)\n",
    "    # print(\"negative\", probNeg)\n",
    "\n",
    "\n",
    "    start = time.process_time()\n",
    "    # #multiply by (add log) the pos prior, which is the other part of our equation in the unigram model\n",
    "    # time to work with the dev set\n",
    "    predictions = []\n",
    "    for x in range(len(dev_set)):\n",
    "        chancePos = math.log(pos_prior)\n",
    "        chanceNeg = math.log(1-pos_prior)\n",
    "        for y in range(len(dev_set[x])):\n",
    "            if dev_set[x][y] in mydict:\n",
    "                chancePos += probWordPos[dev_set[x][y]]\n",
    "                chanceNeg += probWordNeg[dev_set[x][y]]\n",
    "            # else:\n",
    "                # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "                # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        if(chancePos > chanceNeg):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Mixed Model approach (unigram+bigram) ###################\n",
    "# def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "#     \"\"\"\n",
    "#     train_set - List of list of words corresponding with each movie review\n",
    "#     example: suppose I had two reviews 'like this movie' and 'i fall asleep' in my training set\n",
    "#     Then train_set := [['like','this','movie'], ['i','fall','asleep']]\n",
    "\n",
    "#     train_labels - List of labels corresponding with train_set\n",
    "#     example: Suppose I had two reviews, first one was positive and second one was negative.\n",
    "#     Then train_labels := [1, 0]\n",
    "\n",
    "#     dev_set - List of list of words corresponding with each review that we are testing on\n",
    "#               It follows the same format as train_set\n",
    "\n",
    "#     smoothing_parameter - The smoothing parameter you provided with --laplace (1.0 by default)\n",
    "#     \"\"\"\n",
    "#     begin = time.process_time()\n",
    "#     stops = stopwords.words('english') + list(string.punctuation)\n",
    "#     mydict = {}\n",
    "#     mybidict = {}\n",
    "#     smoothing_parameter = 0.21\n",
    "#     smoothing_parameter_bi = 0.75\n",
    "#     posV = 0\n",
    "#     posVbi = 0\n",
    "#     negV = 0\n",
    "#     posVbi = 0\n",
    "#     negVbi = 0\n",
    "#     totalposwords = 0\n",
    "#     totalposwordsbi = 0\n",
    "#     totalnegwords = 0\n",
    "#     totalnegwordsbi = 0\n",
    "\n",
    "#     print(\"creating occurences and wordlist\")\n",
    "#     #create bag of words and number of occurences\n",
    "\n",
    "#     start = time.process_time()\n",
    "#     count = 0\n",
    "#     for x in train_set:\n",
    "#         rating = train_labels[count]\n",
    "#         count += 1\n",
    "#         if(rating):\n",
    "#             for y in x:\n",
    "#                 if y in stops:\n",
    "#                     continue\n",
    "#                 if y not in mydict:\n",
    "#                     mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "#                     posV += 1\n",
    "#                     totalposwords += 1\n",
    "#                 else:\n",
    "#                     if mydict[y][0] == 0:\n",
    "#                         posV += 1\n",
    "#                     mydict[y][0] += 1\n",
    "#                     totalposwords += 1\n",
    "#             for y,z in zip(x, x[1:]):\n",
    "#                 if y in stops or z in stops:\n",
    "#                     continue\n",
    "#                 if (y,z) not in mybidict:\n",
    "#                     mybidict[(y,z)] = [1,0]\n",
    "#                     posVbi += 1\n",
    "#                     totalposwordsbi += 1\n",
    "#                 else:\n",
    "#                     if mybidict[(y,z)][0] == 0:\n",
    "#                         posVbi += 1\n",
    "#                     mybidict[(y,z)][0] += 1\n",
    "#                     totalposwordsbi += 1\n",
    "#                     # print (\"[x,y] : \", [x,y])\n",
    "#         else:\n",
    "#             for y in x:\n",
    "#                 if y in stops:\n",
    "#                     continue\n",
    "#                 if y not in mydict:\n",
    "#                     mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "#                     negV += 1\n",
    "#                     totalnegwords += 1\n",
    "#                 else:\n",
    "#                     if mydict[y][1] == 0:\n",
    "#                         negV += 1\n",
    "#                     mydict[y][1] += 1\n",
    "#                     totalnegwords += 1\n",
    "#             for y,z in zip(x, x[1:]):\n",
    "#                 if y in stops or z in stops:\n",
    "#                     continue\n",
    "#                 if (y,z) not in mybidict:\n",
    "#                     # print(\"(\",y,\",\", z, \")\")\n",
    "#                     mybidict[(y,z)] = [0,1]\n",
    "#                     negVbi += 1\n",
    "#                     totalnegwordsbi += 1\n",
    "#                 else:\n",
    "#                     if mybidict[(y,z)][1] == 0:\n",
    "#                         negVbi += 1\n",
    "#                     mybidict[(y,z)][1] += 1\n",
    "#                     totalnegwordsbi += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "#     print(\"posVbi\", posVbi)\n",
    "#     print(\"negVbi\", negVbi)\n",
    "#     print(\"total bi pair count is:\", totalposwordsbi  + totalnegwordsbi)\n",
    "#     print(\"Going through train took: \", time.process_time() - start)\n",
    "\n",
    "    \n",
    "#     #come up with the bag of words\n",
    "#     probWordPos = {}\n",
    "#     probWordNeg = {}\n",
    "#     # PosII = 0\n",
    "#     # NegII = 0\n",
    "#     # print(\"calculate prob\")\n",
    "#     for x in mydict:\n",
    "#         #use laplace smoothing\n",
    "#         # count(W) + a / n + a * (V+1)\n",
    "#         # n = number of words in our UK training data\n",
    "#         # count(W) = number of times W appeared in UK training data\n",
    "#         # α is a tuning constant between 0 and 1 (typically small)\n",
    "#         # V = number of word TYPES seen in training data\n",
    "\n",
    "#         probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "#         probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "#         #calculate that II symbol that is basically summation, but using mutiplication\n",
    "#         #logs are used because we are working with incredibly small numbers\n",
    "#         # PosII += (probWordPos[x])\n",
    "#         # NegII += (probWordNeg[x])\n",
    "#     # start = time.process_time()\n",
    "#     probPairPos = {}\n",
    "#     probPairNeg = {}\n",
    "#     for x in mybidict:\n",
    "#         probPairPos[x] = math.log((mybidict[x][0] + smoothing_parameter_bi) / (totalposwordsbi + smoothing_parameter_bi * (posVbi + 1)))\n",
    "#         probPairNeg[x] = math.log((mybidict[x][1] + smoothing_parameter_bi) / (totalnegwordsbi + smoothing_parameter_bi * (negVbi + 1)))\n",
    "#     #unneccessary calculations on prob of train set\n",
    "#     # print(\"time to print bi dict\", time.process_time() - start)\n",
    "#     # print(\"PosII\", PosII)\n",
    "#     # print(\"NegII\", NegII)\n",
    "#     # probPos = math.log(pos_prior) + PosII\n",
    "#     # probNeg = math.log(1 - pos_prior) + NegII\n",
    "\n",
    "#     # print(\"positive\", probPos)\n",
    "#     # print(\"negative\", probNeg)\n",
    "    \n",
    "#     start = time.process_time()\n",
    "#     # #multiply by (add log) the pos prior, which is the other part of our equation in the unigram    model\n",
    "#     # time to work with the dev set\n",
    "#     predictions = []\n",
    "#     lambd = 0.475\n",
    "#     lambdaUni = lambd\n",
    "#     lambdaBi = 1 - lambd\n",
    "#     for x in range(len(dev_set)):\n",
    "#         chancePosUni = math.log(pos_prior)\n",
    "#         chanceNegUni = math.log(1-pos_prior)\n",
    "#         chancePosBi = math.log(pos_prior)\n",
    "#         chanceNegBi = math.log(1-pos_prior)\n",
    "#         for y in range(len(dev_set[x])):\n",
    "#             if dev_set[x][y] in mydict:\n",
    "#                 chancePosUni += probWordPos[dev_set[x][y]]\n",
    "#                 chanceNegUni += probWordNeg[dev_set[x][y]]\n",
    "#             # else:\n",
    "#                 # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "#                 # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "#         for y,z in zip(dev_set[x], dev_set[x][1:]):\n",
    "#             if (y,z) in mybidict:\n",
    "#                 chancePosBi += probPairPos[(y,z)]\n",
    "#                 chanceNegBi += probPairNeg[(y,z)]\n",
    "#         chancePos = (lambdaBi * chancePosBi) + (lambdaUni * chancePosUni)\n",
    "#         chanceNeg = (lambdaBi * chanceNegBi) + (lambdaUni * chanceNegUni)\n",
    "#         if(chancePos > chanceNeg):\n",
    "#             predictions.append(1)\n",
    "#         else:\n",
    "#             predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "#     print(\"method took:\", time.process_time() - begin)\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(predictedLabels, dev_set, dev_labels):\n",
    "    yhats = predictedLabels\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    tp = np.sum([yhats[i] == dev_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    precision = tp / np.sum([yhats[i] == 1 for i in range(len(yhats))])\n",
    "    recall = tp / (np.sum([yhats[i] != dev_labels[i] and yhats[i] == 0 for i in range(len(yhats))]) + tp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, stemming, lowerCase, laplace, posPrior):\n",
    "    trainSet, trainLabels, revSet, revLabels = loadDataset(dataset, stemming, lowerCase)\n",
    "    predictedLabels = naiveBayes(trainSet, trainLabels, revSet, laplace, posPrior)\n",
    "\n",
    "    accuracy, f1, precision, recall = compute_accuracies(predictedLabels, revSet, revLabels)\n",
    "#     print(\"Accuracy:\",accuracy)\n",
    "#     print(\"F1-Score:\",f1)\n",
    "#     print(\"Precision:\",precision)\n",
    "#     print(\"Recall:\",recall)\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2666.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3075.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 1 ---------------\n",
      "Accuracy: 0.8\n",
      "F1-Score: 0.7989949748743719\n",
      "Precision: 0.803030303030303\n",
      "Recall: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2364.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2832.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 2 ---------------\n",
      "Accuracy: 0.8325\n",
      "F1-Score: 0.8312342569269522\n",
      "Precision: 0.8375634517766497\n",
      "Recall: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2966.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2931.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 3 ---------------\n",
      "Accuracy: 0.81\n",
      "F1-Score: 0.8080808080808082\n",
      "Precision: 0.8163265306122449\n",
      "Recall: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3066.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3308.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 4 ---------------\n",
      "Accuracy: 0.785\n",
      "F1-Score: 0.788177339901478\n",
      "Precision: 0.7766990291262136\n",
      "Recall: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2709.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3224.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 5 ---------------\n",
      "Accuracy: 0.775\n",
      "F1-Score: 0.7783251231527093\n",
      "Precision: 0.7669902912621359\n",
      "Recall: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2033.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1920.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 6 ---------------\n",
      "Accuracy: 0.8\n",
      "F1-Score: 0.8029556650246306\n",
      "Precision: 0.7912621359223301\n",
      "Recall: 0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3002.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2906.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 7 ---------------\n",
      "Accuracy: 0.7875\n",
      "F1-Score: 0.7880299251870324\n",
      "Precision: 0.7860696517412935\n",
      "Recall: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2785.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3075.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 8 ---------------\n",
      "Accuracy: 0.7575\n",
      "F1-Score: 0.7581047381546135\n",
      "Precision: 0.7562189054726368\n",
      "Recall: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2800.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2914.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 9 ---------------\n",
      "Accuracy: 0.76\n",
      "F1-Score: 0.7611940298507464\n",
      "Precision: 0.7574257425742574\n",
      "Recall: 0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2638.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2975.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN NUMBER 10 ---------------\n",
      "Accuracy: 0.75\n",
      "F1-Score: 0.7487437185929648\n",
      "Precision: 0.7525252525252525\n",
      "Recall: 0.745\n",
      "Final results----------------------------------\n",
      "Average Accuracy: 0.78575\n",
      "Average F1: 0.7863840579746307\n",
      "Average Precision: 0.7844111294043318\n",
      "Average recall 0.7885\n",
      "STD Accuracy: 0.02457259652539798\n",
      "STD F1: 0.024161677220446418\n",
      "STD Precision: 0.026726029727975897\n",
      "STD Recall: 0.023669600757089244\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = \"../TermProject/txt_sentoken\"\n",
    "    stemming = []\n",
    "    lowerCase = True\n",
    "    laplace = 1.0\n",
    "    posPrior = 0.8\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    numberOfRuntimes = 10\n",
    "    for i in range(numberOfRuntimes):\n",
    "        curaccuracy, curf1, curprecision, currecall = main(dataset, stemming, lowerCase, laplace, posPrior)\n",
    "        accuracy.append(curaccuracy)\n",
    "        f1.append(curf1)\n",
    "        precision.append(curprecision)\n",
    "        recall.append(currecall)\n",
    "        print(\"RUN NUMBER \" + str(i+1) + \" ---------------\")\n",
    "        print(\"Accuracy:\",curaccuracy)\n",
    "        print(\"F1-Score:\",curf1)\n",
    "        print(\"Precision:\",curprecision)\n",
    "        print(\"Recall:\",currecall)\n",
    "    aveAccuracy = np.mean(accuracy)\n",
    "    avef1 = np.mean(f1)\n",
    "    avePrecision = np.mean(precision)\n",
    "    aveRecall = np.mean(recall)\n",
    "    stdAccuracy = np.std(accuracy)\n",
    "    stdf1 = np.std(f1)\n",
    "    stdPrecision = np.std(precision)\n",
    "    stdRecall = np.std(recall)\n",
    "    print(\"Final results----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
