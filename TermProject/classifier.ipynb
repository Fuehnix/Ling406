{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk.sentiment.util\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(directory, stemming, lower_case):\n",
    "    positive = loadDir(directory + '/pos/',stemming, lower_case)\n",
    "    negative = loadDir(directory + '/neg/',stemming, lower_case)\n",
    "    random.shuffle(positive)\n",
    "    boundaryTrain = math.floor(0.8 * len(positive))\n",
    "    trainPos = positive[:boundaryTrain]\n",
    "    random.shuffle(negative)\n",
    "    trainNeg = negative[:boundaryTrain]\n",
    "    combinedTrain = trainPos + trainNeg\n",
    "    length = len(trainPos) + len(trainNeg)\n",
    "    labelsTrain = boundaryTrain * [1] + boundaryTrain * [0]\n",
    "    labelsTrain = np.array(labelsTrain)\n",
    "\n",
    "    testPos = positive[boundaryTrain:]\n",
    "    testNeg = negative[boundaryTrain:]\n",
    "    combinedTest = testPos + testNeg\n",
    "    labelsTest = boundaryTrain * [1] + boundaryTrain * [0]\n",
    "    labelsTest = np.array(labelsTest)\n",
    "    print(labelsTrain)\n",
    "    return combinedTrain, labelsTrain, combinedTest, labelsTest, boundaryTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDir(name,stemming,lower_case):\n",
    "    # Loads the files in the folder and returns a list of lists of words from the text in each file\n",
    "    if stemming:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data = []\n",
    "    count = 0\n",
    "    for f in tqdm(listdir(name)):\n",
    "        fullname = name+f\n",
    "        text = []\n",
    "        with open(fullname, 'rb') as f:\n",
    "            for line in f:\n",
    "                if lower_case:\n",
    "                    line = line.decode(errors='ignore').lower()\n",
    "                    text += tokenizer.tokenize(line)\n",
    "                else:\n",
    "                    text += tokenizer.tokenize(line.decode(errors='ignore'))\n",
    "        if stemming:\n",
    "            for i in range(len(text)):\n",
    "#                 if text[i] in bad_words:\n",
    "#                     continue\n",
    "                text[i] = porter_stemmer.stem(text[i])\n",
    "        data.append(text)\n",
    "        count = count + 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_svm_featureset(neg_bow,pos_bow):\n",
    "    review_word_index = []\n",
    "    for word in neg_bow.keys():\n",
    "        review_word_index.append(word)\n",
    "    for word in pos_bow.keys():\n",
    "        review_word_index.append(word)\n",
    "    review_word_index = sorted(list(set(review_word_index)))\n",
    "    return review_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords(train_set, train_labels):\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    count = 0\n",
    "    mydict = {}\n",
    "    posV = 0\n",
    "    negV = 0\n",
    "    totalposwords = 0\n",
    "    totalnegwords = 0\n",
    "    for x in train_set:\n",
    "        rating = train_labels[count]\n",
    "        count += 1\n",
    "        if(rating):\n",
    "            for w in x:\n",
    "                if w not in mydict and w not in stops:\n",
    "                    mydict[w] = [1,0] #default [1 pos, 0 neg]\n",
    "                    posV += 1\n",
    "                    totalposwords += 1\n",
    "                elif w not in stops:\n",
    "                    if mydict[w][0] == 0:\n",
    "                        posV += 1\n",
    "                    mydict[w][0] += 1\n",
    "                    totalposwords += 1\n",
    "        else:\n",
    "            for w in x:\n",
    "                if w not in mydict and w not in stops:\n",
    "                    mydict[w] = [0,1] #default [0 pos, 1 neg]\n",
    "                    negV += 1\n",
    "                    totalnegwords += 1\n",
    "                elif y not in stops:\n",
    "                    if mydict[w][1] == 0:\n",
    "                        negV += 1\n",
    "                    mydict[w][1] += 1\n",
    "                    totalnegwords += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "    BOW = mydict, posV, negV, totalposwords, totalnegwords\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "    #Baseline#\n",
    "    # return predicted labels of development set\n",
    "    # print(\"not even started yet\")\n",
    "    smoothing_parameter = 0.034\n",
    "\n",
    "#     start = time.process_time()\n",
    "\n",
    "#     print(\"Going through train set took: \", time.process_time() - start)\n",
    "    mydictPos,mydictNeg, posV, negV, totalposwords, totalnegwords = bagOfWords(train_set, train_labels)\n",
    "    \n",
    "    \n",
    "    #come up with the bag of words unigram model\n",
    "    probWordPos = {}\n",
    "    probWordNeg = {}\n",
    "#     start = time.process_time()\n",
    "#     print(\"calculate prob\")\n",
    "    for x in mydict:\n",
    "        #use laplace smoothing\n",
    "        # count(W) + a / n + a * (V+1)\n",
    "        # n = number of words in our UK training data\n",
    "        # count(W) = number of times W appeared in UK training data\n",
    "        # α is a tuning constant between 0 and 1 (typically small)\n",
    "        # V = number of word TYPES seen in training data\n",
    "\n",
    "        probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "        probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        #calculate that II symbol that is basically summation, but using mutiplication\n",
    "        #logs are used because we are working with incredibly small numbers\n",
    "        # PosII += (probWordPos[x])\n",
    "        # NegII += (probWordNeg[x])\n",
    "    \n",
    "\n",
    "    #unneccessary calculations on prob of train set\n",
    "#     print(\"Prob calculations: \", time.process_time() - start)\n",
    "    # print(\"PosII\", PosII)\n",
    "    # print(\"NegII\", NegII)\n",
    "    # probPos = math.log(pos_prior) + PosII\n",
    "    # probNeg = math.log(1 - pos_prior) + NegII\n",
    "\n",
    "    # print(\"positive\", probPos)\n",
    "    # print(\"negative\", probNeg)\n",
    "\n",
    "\n",
    "    start = time.process_time()\n",
    "    # #multiply by (add log) the pos prior, which is the other part of our equation in the unigram model\n",
    "    # time to work with the dev set\n",
    "    predictions = []\n",
    "    for x in range(len(dev_set)):\n",
    "        chancePos = math.log(pos_prior)\n",
    "        chanceNeg = math.log(1-pos_prior)\n",
    "        for y in range(len(dev_set[x])):\n",
    "            if dev_set[x][y] in mydict:\n",
    "                chancePos += probWordPos[dev_set[x][y]]\n",
    "                chanceNeg += probWordNeg[dev_set[x][y]]\n",
    "            # else:\n",
    "                # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "                # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "        if(chancePos > chanceNeg):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Mixed Model approach (unigram+bigram) ###################\n",
    "# def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "#     \"\"\"\n",
    "#     train_set - List of list of words corresponding with each movie review\n",
    "#     example: suppose I had two reviews 'like this movie' and 'i fall asleep' in my training set\n",
    "#     Then train_set := [['like','this','movie'], ['i','fall','asleep']]\n",
    "\n",
    "#     train_labels - List of labels corresponding with train_set\n",
    "#     example: Suppose I had two reviews, first one was positive and second one was negative.\n",
    "#     Then train_labels := [1, 0]\n",
    "\n",
    "#     dev_set - List of list of words corresponding with each review that we are testing on\n",
    "#               It follows the same format as train_set\n",
    "\n",
    "#     smoothing_parameter - The smoothing parameter you provided with --laplace (1.0 by default)\n",
    "#     \"\"\"\n",
    "#     begin = time.process_time()\n",
    "#     stops = stopwords.words('english') + list(string.punctuation)\n",
    "#     mydict = {}\n",
    "#     mybidict = {}\n",
    "#     smoothing_parameter = 0.21\n",
    "#     smoothing_parameter_bi = 0.75\n",
    "#     posV = 0\n",
    "#     posVbi = 0\n",
    "#     negV = 0\n",
    "#     posVbi = 0\n",
    "#     negVbi = 0\n",
    "#     totalposwords = 0\n",
    "#     totalposwordsbi = 0\n",
    "#     totalnegwords = 0\n",
    "#     totalnegwordsbi = 0\n",
    "\n",
    "#     print(\"creating occurences and wordlist\")\n",
    "#     #create bag of words and number of occurences\n",
    "\n",
    "#     start = time.process_time()\n",
    "#     count = 0\n",
    "#     for x in train_set:\n",
    "#         rating = train_labels[count]\n",
    "#         count += 1\n",
    "#         if(rating):\n",
    "#             for y in x:\n",
    "#                 if y in stops:\n",
    "#                     continue\n",
    "#                 if y not in mydict:\n",
    "#                     mydict[y] = [1,0] #default [1 pos, 0 neg]\n",
    "#                     posV += 1\n",
    "#                     totalposwords += 1\n",
    "#                 else:\n",
    "#                     if mydict[y][0] == 0:\n",
    "#                         posV += 1\n",
    "#                     mydict[y][0] += 1\n",
    "#                     totalposwords += 1\n",
    "#             for y,z in zip(x, x[1:]):\n",
    "#                 if y in stops or z in stops:\n",
    "#                     continue\n",
    "#                 if (y,z) not in mybidict:\n",
    "#                     mybidict[(y,z)] = [1,0]\n",
    "#                     posVbi += 1\n",
    "#                     totalposwordsbi += 1\n",
    "#                 else:\n",
    "#                     if mybidict[(y,z)][0] == 0:\n",
    "#                         posVbi += 1\n",
    "#                     mybidict[(y,z)][0] += 1\n",
    "#                     totalposwordsbi += 1\n",
    "#                     # print (\"[x,y] : \", [x,y])\n",
    "#         else:\n",
    "#             for y in x:\n",
    "#                 if y in stops:\n",
    "#                     continue\n",
    "#                 if y not in mydict:\n",
    "#                     mydict[y] = [0,1] #default [0 pos, 1 neg]\n",
    "#                     negV += 1\n",
    "#                     totalnegwords += 1\n",
    "#                 else:\n",
    "#                     if mydict[y][1] == 0:\n",
    "#                         negV += 1\n",
    "#                     mydict[y][1] += 1\n",
    "#                     totalnegwords += 1\n",
    "#             for y,z in zip(x, x[1:]):\n",
    "#                 if y in stops or z in stops:\n",
    "#                     continue\n",
    "#                 if (y,z) not in mybidict:\n",
    "#                     # print(\"(\",y,\",\", z, \")\")\n",
    "#                     mybidict[(y,z)] = [0,1]\n",
    "#                     negVbi += 1\n",
    "#                     totalnegwordsbi += 1\n",
    "#                 else:\n",
    "#                     if mybidict[(y,z)][1] == 0:\n",
    "#                         negVbi += 1\n",
    "#                     mybidict[(y,z)][1] += 1\n",
    "#                     totalnegwordsbi += 1\n",
    "#     print(\"review count is: \", count)\n",
    "#     print(\"posV\", posV)\n",
    "#     print(\"negV\", negV)\n",
    "#     print(\"total word count is:\", totalposwords + totalnegwords)\n",
    "#     print(\"posVbi\", posVbi)\n",
    "#     print(\"negVbi\", negVbi)\n",
    "#     print(\"total bi pair count is:\", totalposwordsbi  + totalnegwordsbi)\n",
    "#     print(\"Going through train took: \", time.process_time() - start)\n",
    "\n",
    "    \n",
    "#     #come up with the bag of words\n",
    "#     probWordPos = {}\n",
    "#     probWordNeg = {}\n",
    "#     # PosII = 0\n",
    "#     # NegII = 0\n",
    "#     # print(\"calculate prob\")\n",
    "#     for x in mydict:\n",
    "#         #use laplace smoothing\n",
    "#         # count(W) + a / n + a * (V+1)\n",
    "#         # n = number of words in our UK training data\n",
    "#         # count(W) = number of times W appeared in UK training data\n",
    "#         # α is a tuning constant between 0 and 1 (typically small)\n",
    "#         # V = number of word TYPES seen in training data\n",
    "\n",
    "#         probWordPos[x] = math.log((mydict[x][0] + smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "#         probWordNeg[x] = math.log((mydict[x][1] + smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "#         #calculate that II symbol that is basically summation, but using mutiplication\n",
    "#         #logs are used because we are working with incredibly small numbers\n",
    "#         # PosII += (probWordPos[x])\n",
    "#         # NegII += (probWordNeg[x])\n",
    "#     # start = time.process_time()\n",
    "#     probPairPos = {}\n",
    "#     probPairNeg = {}\n",
    "#     for x in mybidict:\n",
    "#         probPairPos[x] = math.log((mybidict[x][0] + smoothing_parameter_bi) / (totalposwordsbi + smoothing_parameter_bi * (posVbi + 1)))\n",
    "#         probPairNeg[x] = math.log((mybidict[x][1] + smoothing_parameter_bi) / (totalnegwordsbi + smoothing_parameter_bi * (negVbi + 1)))\n",
    "#     #unneccessary calculations on prob of train set\n",
    "#     # print(\"time to print bi dict\", time.process_time() - start)\n",
    "#     # print(\"PosII\", PosII)\n",
    "#     # print(\"NegII\", NegII)\n",
    "#     # probPos = math.log(pos_prior) + PosII\n",
    "#     # probNeg = math.log(1 - pos_prior) + NegII\n",
    "\n",
    "#     # print(\"positive\", probPos)\n",
    "#     # print(\"negative\", probNeg)\n",
    "    \n",
    "#     start = time.process_time()\n",
    "#     # #multiply by (add log) the pos prior, which is the other part of our equation in the unigram    model\n",
    "#     # time to work with the dev set\n",
    "#     predictions = []\n",
    "#     lambd = 0.475\n",
    "#     lambdaUni = lambd\n",
    "#     lambdaBi = 1 - lambd\n",
    "#     for x in range(len(dev_set)):\n",
    "#         chancePosUni = math.log(pos_prior)\n",
    "#         chanceNegUni = math.log(1-pos_prior)\n",
    "#         chancePosBi = math.log(pos_prior)\n",
    "#         chanceNegBi = math.log(1-pos_prior)\n",
    "#         for y in range(len(dev_set[x])):\n",
    "#             if dev_set[x][y] in mydict:\n",
    "#                 chancePosUni += probWordPos[dev_set[x][y]]\n",
    "#                 chanceNegUni += probWordNeg[dev_set[x][y]]\n",
    "#             # else:\n",
    "#                 # chancePos += math.log((smoothing_parameter) / (totalposwords + smoothing_parameter * (posV + 1)))\n",
    "#                 # chanceNeg += math.log((smoothing_parameter) / (totalnegwords + smoothing_parameter * (negV + 1)))\n",
    "#         for y,z in zip(dev_set[x], dev_set[x][1:]):\n",
    "#             if (y,z) in mybidict:\n",
    "#                 chancePosBi += probPairPos[(y,z)]\n",
    "#                 chanceNegBi += probPairNeg[(y,z)]\n",
    "#         chancePos = (lambdaBi * chancePosBi) + (lambdaUni * chancePosUni)\n",
    "#         chanceNeg = (lambdaBi * chanceNegBi) + (lambdaUni * chanceNegUni)\n",
    "#         if(chancePos > chanceNeg):\n",
    "#             predictions.append(1)\n",
    "#         else:\n",
    "#             predictions.append(0)\n",
    "#     print(\"devset time took:\", time.process_time() - start)\n",
    "#     print(\"method took:\", time.process_time() - begin)\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(train_set, train_labels, dev_set):\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    cv = CountVectorizer(tokenizer = tokenizer, stop_words = stops)\n",
    "    LRclassifier = LogisticRegression()\n",
    "    mydict, posV, negV, totalposwords, totalnegwords = bagOfWords(train_set, train_labels)\n",
    "    trainSet = cv.fit_transform(mydict)\n",
    "#     for i in range(len(train_labels)):\n",
    "#         labelSet += (fillTrainset(train_set[i],train_labels[i]))\n",
    "#         trainSet += train_set[i]\n",
    "#     print(labelSet)\n",
    "#     print(train_set)\n",
    "#     print(trainSet)\n",
    "    print(train_set[0])\n",
    "    labels = [0]*750+[1]*750\n",
    "    LRclassifier.fit(train_set,train_labels)\n",
    "    predictions = LRclassifier.predict(dev_set)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportVectorMachine(train_set, train_labels, dev_set):\n",
    "    SVMclassifier = LinearSVC()\n",
    "    SVMclassifier.fit(train_set,train_labels)\n",
    "    predictions = SVMclassifier.predict(dev_set)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree(train_set, train_labels, dev_set):\n",
    "    DTclassifier = DecisionTreeClassifier()\n",
    "    DTclassifier.fit(train_set,train_labels)\n",
    "    predictions = DTclassifier.predict(dev_set)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(predictedLabels, dev_set, dev_labels):\n",
    "    yhats = predictedLabels\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    tp = np.sum([yhats[i] == dev_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    precision = tp / np.sum([yhats[i] == 1 for i in range(len(yhats))])\n",
    "    recall = tp / (np.sum([yhats[i] != dev_labels[i] and yhats[i] == 0 for i in range(len(yhats))]) + tp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(train_set, train_labels, dev_set, dev_labels):\n",
    "    labelSet = []\n",
    "    trainSet = []\n",
    "    dataSet = []\n",
    "    dataLabels = []\n",
    "    for i in range(len(train_labels)):\n",
    "        labelSet += (fillTrainset(train_set[i],train_labels[i]))\n",
    "        trainSet += train_set[i]\n",
    "    dataSet = train_set + dev_set\n",
    "    dataLabels = train_labels + dev_labels\n",
    "    trainSet, trainLabels, revSet, revLabels = train_test_split(dataSet, dataLabels, test_size=0.2)\n",
    "    return trainSet, trainLabels, revSet, revLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(line, stop=True, negation=True):\n",
    "    line = line.split(\" \")\n",
    "    stops = stopwords.words('english') + list(string.punctuation)\n",
    "    if stop:\n",
    "        line = [word for word in line if word not in set(stops)]\n",
    "    if negation:\n",
    "        line = nltk.sentiment.util.mark_negation(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTrainset(review, label):\n",
    "    labelSet = []\n",
    "    if label == 0:\n",
    "        labelSet = [0]*len(review)\n",
    "    if label == 1:\n",
    "        labelSet = [1]*len(review)\n",
    "    return labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, stemming, lowerCase, laplace, posPrior):\n",
    "    trainSet, trainLabels, revSet, revLabels, boundary = loadDataset(dataset, stemming, lowerCase)\n",
    "    mydict, posV, negV, totalposwords, totalnegwords = bagOfWords(trainSet, trainLabels)\n",
    "#     print(mydict)\n",
    "    print(len(trainSet))\n",
    "    print(len(trainLabels))\n",
    "    print(trainSet[0])\n",
    "    print(trainSet[1])\n",
    "    print(trainLabels[0])\n",
    "    print(trainLabels[1])\n",
    "#     trainSet, trainLabels, revSet, revLabels = splitData(trainSet, trainLabels, revSet, revLabels)\n",
    "\n",
    "    predictedLabelsNB = naiveBayes(trainSet, trainLabels, revSet, laplace, posPrior)\n",
    "    predictedLabelsLR = logisticRegression(trainSet, trainLabels, revSet)\n",
    "    predictedLabelsSVM = supportVectorMachine(trainSet, trainLabels, revSet)\n",
    "    predictedLabelsDT = decisionTree(trainSet, trainLabels, revSet)\n",
    "    \n",
    "    accuracyNB, f1NB, precisionNB, recallNB = compute_accuracies(predictedLabelsNB, revSet, revLabels)\n",
    "    accuracyLR, f1LR, precisionLR, recallLR = compute_accuracies(predictedLabelsLR, revSet, revLabels)\n",
    "    accuracySVM, f1SVM, precisionSVM, recallSVM = compute_accuracies(predictedLabelsSVM, revSet, revLabels)\n",
    "    accuracyDT, f1DT, precisionDT, recallDT = compute_accuracies(predictedLabelsDT, revSet, revLabels)\n",
    "    \n",
    "    NBscores = accuracyNB, f1NB, precisionNB, recallNB\n",
    "    LRscores = accuracyLR, f1LR, precisionLR, recallLR\n",
    "    SVMscores = accuracySVM, f1SVM, precisionSVM, recallSVM\n",
    "    DTscores = accuracyDT, f1DT, precisionDT, recallDT\n",
    "#     print(\"Accuracy:\",accuracy)\n",
    "#     print(\"F1-Score:\",f1)\n",
    "#     print(\"Precision:\",precision)\n",
    "#     print(\"Recall:\",recall)\n",
    "    return NBscores, LRscores, SVMscores, DTscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1863.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1881.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n",
      "1600\n",
      "1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'some', 'may', 'consider', 'portions', 'of', 'the', 'following', 'text', 'to', 'be', 'spoilers', 'be', 'forewarned', 'milos', 'forman', 's', 'first', 'film', 'since', 'the', 'ill', 'fated', 'valmont', 'columbia', 's', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'a', 'vastly', 'entertaining', 'if', 'not', 'particularly', 'enlightening', 'biopic', 'of', 'hustler', 'publisher', 'and', 'self', 'made', 'millionaire', 'larry', 'flynt', 'who', 'became', 'an', 'unlikely', 'champion', 'of', 'freedom', 'of', 'speech', 'rights', 'in', 'the', 'united', 'states', 'in', 'the', 'late', '1970s', 'and', 'early', '1980s', 'the', 'film', 'unweaves', 'its', 'tale', 'in', 'a', 'chronological', 'order', 'we', 'open', 'with', 'young', 'and', 'dirt', 'poor', 'larry', 'flynt', 'and', 'his', 'brother', 'jimmy', 'peddling', 'jars', 'of', 'water', 'in', 'true', 'entrepreneurial', 'spirit', 'out', 'in', 'the', 'rural', 'outback', 'of', 'kentucky', 'cut', 'to', 'forward', 'in', 'time', 'where', 'the', 'two', 'flynt', 'brothers', 'now', 'young', 'men', 'are', 'running', 'the', 'struggling', 'hustler', 'go', 'go', 'clubs', 'in', 'cincinnati', 'the', 'strip', 'clubs', 'are', 'in', 'a', 'dire', 'financial', 'state', 'and', 'in', 'a', 'last', 'ditch', 'effort', 'to', 'salvage', 'the', 'operations', 'flynt', 'decides', 'to', 'go', 'to', 'a', 'print', 'shop', 'and', 'churn', 'out', 'a', 'promotional', 'newsletter', 'this', 'evolved', 'into', 'the', 'adult', 'periodical', '_hustler_', 'magazine', 'creating', 'larry', 'flynt', 'a', 'vast', 'financial', 'empire', 'and', 'the', 'rest', 'is', 'history', 'what', 'sets', 'flynt', 'apart', 'from', 'other', 'publishers', 'is', 'his', 'struggles', 'against', 'those', 'who', 'would', 'have', 'him', 'cease', 'publication', 'of', 'his', 'adult', 'material', 'and', 'who', 'railed', 'and', 'preached', 'against', 'him', 'flynt', 'spent', 'time', 'in', 'incarceration', 'and', 'was', 'paralyzed', 'by', 'an', 'assassination', 'attempt', 'and', 'his', 'driven', 'single', 'minded', 'insistence', 'to', 'buck', 'the', 'system', 'and', 'fight', 'for', 'his', 'freedom', 'of', 'expression', 'regardless', 'of', 'personal', 'cost', 'the', 'people', 'vs', 'larry', 'flynt', 'also', 'weaves', 'in', 'the', 'bittersweet', 'story', 'of', 'flynt', 's', 'true', 'love', 'althea', 'leasure', 'whom', 'he', 'meets', 'as', 'a', 'dancer', 'in', 'his', 'club', 'and', 'later', 'marries', 'and', 'who', 'devotedly', 'stands', 'alongside', 'him', 'throughout', 'his', 'trials', 'and', 'tribulations', 'considering', 'the', 'serious', 'nature', 'of', 'the', 'film', 's', 'theme', 'the', 'importance', 'of', 'the', 'united', 'states', 'first', 'amendment', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'surprisingly', 'and', 'wonderfully', 'light', 'hearted', 'and', 'humourous', 'much', 'of', 'the', 'comedy', 'is', 'elicited', 'from', 'larry', 'flynt', 's', 'outlandish', 'stunts', 'at', 'his', 'courtroom', 'appearances', 'some', 'of', 'his', 'chosen', 'apparel', 'is', 'hilarious', 'and', 'for', 'the', 'most', 'part', 'these', 'elements', 'of', 'the', 'film', 'work', 'far', 'better', 'than', 'some', 'of', 'the', 'more', 'dramatic', 'points', 'such', 'as', 'an', 'uninspiring', 'flynt', 'monologue', 'set', 'at', 'a', 'free', 'speech', 'rally', 'in', 'front', 'of', 'an', 'enormous', 'american', 'flag', 'dealing', 'with', 'the', 'subjectivity', 'of', 'obscenity', 'the', 'film', 's', 'focus', 'is', 'on', 'the', 'flynt', 's', 'many', 'battles', 'over', 'first', 'amendment', 'rights', 'and', 'freedom', 'of', 'speech', 'but', 'the', 'heart', 'of', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'the', 'touching', 'love', 'story', 'between', 'flynt', 'and', 'althea', 'larry', 'flynt', 'is', 'shown', 'as', 'being', 'occassionally', 'gruff', 'harsh', 'and', 'overtly', 'aggressive', 'with', 'his', 'friends', 'and', 'colleagues', 'but', 'with', 'althea', 'we', 'see', 'his', 'loving', 'affectionate', 'side', 'there', 's', 'a', 'scene', 'where', 'flynt', 'tenderly', 'takes', 'his', 'ill', 'wife', 'on', 'a', 'ride', 'on', 'his', 'wheelchair', 'that', 'is', 'heartbreaking', 'ultimately', 'the', 'emotional', 'power', 'that', 'the', 'film', 'hits', 'at', 'its', 'conclusion', 'comes', 'not', 'from', 'his', 'achievements', 'from', 'his', 'battles', 'against', 'censors', 'but', 'from', 'the', 'strength', 'of', 'flynt', 'and', 'althea', 's', 'love', 'for', 'each', 'other', 'woody', 'harrelson', 'is', 'entirely', 'engaging', 'in', 'what', 'must', 'be', 'certainly', 'a', 'career', 'topping', 'performance', 'as', 'the', 'irrepressible', 'larry', 'flynt', 'harrelson', 'plays', 'flynt', 'with', 'the', 'right', 'mixture', 'of', 'outrageousness', 'and', 'confident', 'stubborness', 'to', 'make', 'him', 'endearing', 'and', 'entirely', 'sympathetic', 'to', 'the', 'audience', 'and', 'a', 'very', 'compelling', 'protagonist', 'for', 'the', 'film', 'courtney', 'love', 'plays', 'althea', 'leasure', 'in', 'a', 'startling', 'turn', 'completely', 'raw', 'and', 'impulsive', 'it', 's', 'a', 'very', 'solid', 'performance', 'brash', 'and', 'naturalistic', 'and', 'love', 'is', 'extremely', 'compelling', 'it', 's', 'difficult', 'to', 'take', 'your', 'eyes', 'off', 'her', 'onscreen', 'and', 'her', 'chemistry', 'with', 'harrelson', 'is', 'dead', 'on', 'edward', 'norton', 'as', 'flynt', 's', 'straight', 'level', 'headed', 'lawyer', 'is', 'often', 'upstaged', 'by', 'his', 'flashier', 'co', 'stars', 'in', 'the', 'people', 'vs', 'larry', 'flynt', 'much', 'as', 'his', 'counterpart', 'lawyer', 'alan', 'isaacman', 'was', 'upstaged', 'by', 'flynt', 'during', 'many', 'of', 'the', 'courtroom', 'scenes', 'but', 'norton', 'shines', 'in', 'his', 'big', 'scene', 'where', 'he', 'addresses', 'the', 'supreme', 'court', 'in', 'the', 'climactic', 'scene', 'of', 'the', 'film', 'one', 'can', 'sense', 'the', 'frustration', 'that', 'norton', 's', 'character', 'feels', 'when', 'harrelson', 's', 'free', 'talking', 'flynt', 'sabotages', 'trial', 'after', 'trial', 'on', 'him', 'by', 'openly', 'speaking', 'his', 'mind', 'and', 'this', 'results', 'in', 'a', 'heightened', 'emotional', 'punch', 'when', 'norton', 's', 'isaacman', 'has', 'the', 'opportunity', 'to', 'sway', 'the', 'supreme', 'court', 'judges', 'milos', 'forman', 'keeps', 'the', 'film', 'moving', 'although', 'it', 'runs', 'over', 'two', 'hours', 'it', 'never', 'drags', 'and', 'his', 'direction', 'of', 'the', 'film', 'is', 'very', 'effective', 'eliciting', 'a', 'great', 'deal', 'of', 'empathy', 'for', 'a', 'subject', 'which', 'could', 'be', 'construed', 'by', 'some', 'as', 'extremely', 'sordid', 'and', 'unsympathetic', 'there', 's', 'also', 'a', 'great', 'visual', 'technique', 'which', 'forman', 'uses', 'to', 'indicate', 'the', 'passing', 'of', 'time', 'in', 'one', 'shot', 'which', 'is', 'both', 'clever', 'and', 'extremely', 'entertaining', 'two', 'minor', 'quibbles', 'with', 'the', 'film', 'it', 'certainly', 'seems', 'like', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'in', 'a', 'rush', 'to', 'get', 'to', 'its', 'main', 'theme', 'with', 'flynt', 'battling', 'against', 'authorities', 'and', 'the', 'system', 'for', 'his', 'freedom', 'of', 'speech', 'consequently', 'the', 'first', 'thirty', 'minutes', 'of', 'the', 'film', 'introducing', 'and', 'setting', 'up', 'the', 'characters', 'seem', 'unduly', 'rushed', 'perhaps', 'it', 'is', 'merely', 'due', 'to', 'the', 'fact', 'that', 'these', 'characters', 'are', 'so', 'interesting', 'but', 'i', 'felt', 'it', 'would', 'have', 'worked', 'better', 'if', 'this', 'route', 'was', 'taken', 'in', 'a', 'more', 'leisurely', 'fashion', 'it', 'also', 'felt', 'like', 'there', 'was', 'a', 'distinctive', 'lack', 'of', 'insight', 'into', 'the', 'inner', 'workings', 'of', 'these', 'characters', 'the', 'film', 'clearly', 'shows', 'what', 'flynt', 'althea', 'isaacman', 'and', 'rev', 'jerry', 'faldwell', 'did', 'and', 'on', 'a', 'superficial', 'level', 'some', 'of', 'their', 'motivations', 'but', 'it', 'never', 'seemed', 'like', 'one', 'could', 'really', 'understand', 'the', 'characters', 'on', 'a', 'deeper', 'level', 'for', 'example', 'why', 'larry', 'flynt', 'was', 'compelled', 'by', 'ruth', 'carter', 'stapleton', 'president', 'carter', 's', 'sister', 'to', 'be', 'born', 'again', 'is', 'a', 'mystery', 'to', 'me', 'then', 'again', 'perhaps', 'it', 'was', 'to', 'him', 'as', 'well', 'these', 'two', 'points', 'don', 't', 'detract', 'greatly', 'from', 'the', 'film', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'certainly', 'among', 'the', 'very', 'best', 'studio', 'released', 'films', 'of', '1996', 'and', 'works', 'both', 'as', 'a', 'ringing', 'political', 'statement', 'about', 'the', 'importance', 'of', 'freedom', 'of', 'speech', 'and', 'the', 'depths', 'to', 'which', 'larry', 'flynt', 'would', 'go', 'to', 'advance', 'the', 'cause', 'of', 'free', 'expression', 'and', 'as', 'a', 'touching', 'love', 'story']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'thought', 'baz', 'luhrmann', 's', 'radical', 'take', 'on', '_william_shakespeare', 's_romeo_', '_juliet_', 'was', 'wild', 'wait', 'until', 'what', 'you', 'see', 'what', 'tony', 'award', 'winning', 'stage', 'director', 'julie', 'taymor', '_the_lion_king', '_the_broadway_musical_', 'does', 'to', 'the', 'bard', 's', '_titus_andronicus_', 'for', 'her', 'audacious', 'and', 'very', 'bloody', 'film', 'debut', 'while', 'luhrmann', 'transplanted', 'the', 'star', 'crossed', 'lovers', 'to', '1990s', 'florida', 'taymor', 'keeps', 'this', 'grisly', 'tale', 'of', 'revenge', 'in', 'rome', 'but', 'places', 'it', 'in', 'a', 'surreal', 'temporal', 'limbo', 'the', 'colosseum', 'suggests', 'the', 'proper', 'time', 'of', 'imperial', 'rome', 'but', 'high', 'rises', 'and', 'heavy', 'duty', 'kitchen', 'ovens', 'suggest', 'the', '1990s', 'while', 'the', 'vintage', 'automobiles', 'and', 'some', 'costumes', 'are', 'more', '40s', 'and', '50s', 'and', 'other', 'garments', 'are', 'more', 'gladiator', 'like', 'as', 'tamora', 'queen', 'of', 'goths', 'jessica', 'lange', 'exacts', 'revenge', 'on', 'roman', 'general', 'titus', 'anthony', 'hopkins', 'for', 'killing', 'her', 'eldest', 'son', 'and', 'he', 'does', 'the', 'same', 'to', 'her', 'for', 'the', 'wrongs', 'she', 'commits', 'taymor', 'obviously', 'wanted', 'to', 'express', 'the', 'timeless', 'relevance', 'of', 'the', 'story', 's', 'underlying', 'themes', 'however', 'the', 'time', 'convergence', 'approach', 'only', 'works', 'at', 'times', 'for', 'often', 'it', 's', 'just', 'distracting', 'as', 'in', 'one', 'scene', 'where', 'tamora', 's', 'suviving', 'sons', 'matthew', 'rhys', 'and', 'jonathan', 'rhys', 'meyers', 'blow', 'off', 'steam', 'playing', 'video', 'arcade', 'games', 'taymor', 'didn', 't', 'have', 'to', 'resort', 'to', 'such', 'avant', 'garde', 'time', 'tricks', 'for', 'the', 'story', 'would', 'have', 'resonated', 'just', 'as', 'strongly', 'had', 'she', 'jettisoned', 'them', 'she', 'is', 'a', 'strong', 'storyteller', 'and', 'a', 'master', 'visual', 'stylist', 'the', 'latter', 'compliment', 'encompassing', 'all', 'areas', 'makeup', 'and', 'costume', 'choices', 'as', 'well', 'as', 'those', 'in', 'the', 'editing', 'and', 'photography', 'departments', 'she', 'also', 'has', 'a', 'sure', 'way', 'with', 'actors', 'hopkins', 'titus', 'is', 'at', 'once', 'tragic', 'and', 'horrifying', 'and', 'lange', 'is', 'sultry', 'as', 'the', 'viperous', 'tamora', 'the', 'clear', 'standout', 'of', 'the', 'cast', 'however', 'is', 'harry', 'lennix', 'as', 'aaron', 'a', 'moor', 'who', 'is', 'tamora', 's', 'secret', 'lover', 'and', 'a', 'schemer', 'in', 'his', 'own', 'right', 'lennix', 'brings', 'great', 'depth', 'to', 'a', 'role', 'that', 'could', 'have', 'easily', 'been', 'played', 'as', 'a', 'stock', 'villain', 'a', 'great', 'villain', 'he', 'indeed', 'is', 'but', 'to', 'leave', 'it', 'at', 'that', 'isn', 't', 'giving', 'full', 'justice', 'to', 'his', 'powerful', 'and', 'multi', 'dimensional', 'work', '_titus_', 'isn', 't', 'a', 'complete', 'success', 'but', 'it', 'is', 'never', 'less', 'than', 'fascinating', 'and', 'it', 'announces', 'the', 'arrival', 'of', 'a', 'fearlessly', 'imaginative', 'new', 'cinematic', 'voice']\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'some', 'may', 'consider', 'portions', 'of', 'the', 'following', 'text', 'to', 'be', 'spoilers', 'be', 'forewarned', 'milos', 'forman', 's', 'first', 'film', 'since', 'the', 'ill', 'fated', 'valmont', 'columbia', 's', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'a', 'vastly', 'entertaining', 'if', 'not', 'particularly', 'enlightening', 'biopic', 'of', 'hustler', 'publisher', 'and', 'self', 'made', 'millionaire', 'larry', 'flynt', 'who', 'became', 'an', 'unlikely', 'champion', 'of', 'freedom', 'of', 'speech', 'rights', 'in', 'the', 'united', 'states', 'in', 'the', 'late', '1970s', 'and', 'early', '1980s', 'the', 'film', 'unweaves', 'its', 'tale', 'in', 'a', 'chronological', 'order', 'we', 'open', 'with', 'young', 'and', 'dirt', 'poor', 'larry', 'flynt', 'and', 'his', 'brother', 'jimmy', 'peddling', 'jars', 'of', 'water', 'in', 'true', 'entrepreneurial', 'spirit', 'out', 'in', 'the', 'rural', 'outback', 'of', 'kentucky', 'cut', 'to', 'forward', 'in', 'time', 'where', 'the', 'two', 'flynt', 'brothers', 'now', 'young', 'men', 'are', 'running', 'the', 'struggling', 'hustler', 'go', 'go', 'clubs', 'in', 'cincinnati', 'the', 'strip', 'clubs', 'are', 'in', 'a', 'dire', 'financial', 'state', 'and', 'in', 'a', 'last', 'ditch', 'effort', 'to', 'salvage', 'the', 'operations', 'flynt', 'decides', 'to', 'go', 'to', 'a', 'print', 'shop', 'and', 'churn', 'out', 'a', 'promotional', 'newsletter', 'this', 'evolved', 'into', 'the', 'adult', 'periodical', '_hustler_', 'magazine', 'creating', 'larry', 'flynt', 'a', 'vast', 'financial', 'empire', 'and', 'the', 'rest', 'is', 'history', 'what', 'sets', 'flynt', 'apart', 'from', 'other', 'publishers', 'is', 'his', 'struggles', 'against', 'those', 'who', 'would', 'have', 'him', 'cease', 'publication', 'of', 'his', 'adult', 'material', 'and', 'who', 'railed', 'and', 'preached', 'against', 'him', 'flynt', 'spent', 'time', 'in', 'incarceration', 'and', 'was', 'paralyzed', 'by', 'an', 'assassination', 'attempt', 'and', 'his', 'driven', 'single', 'minded', 'insistence', 'to', 'buck', 'the', 'system', 'and', 'fight', 'for', 'his', 'freedom', 'of', 'expression', 'regardless', 'of', 'personal', 'cost', 'the', 'people', 'vs', 'larry', 'flynt', 'also', 'weaves', 'in', 'the', 'bittersweet', 'story', 'of', 'flynt', 's', 'true', 'love', 'althea', 'leasure', 'whom', 'he', 'meets', 'as', 'a', 'dancer', 'in', 'his', 'club', 'and', 'later', 'marries', 'and', 'who', 'devotedly', 'stands', 'alongside', 'him', 'throughout', 'his', 'trials', 'and', 'tribulations', 'considering', 'the', 'serious', 'nature', 'of', 'the', 'film', 's', 'theme', 'the', 'importance', 'of', 'the', 'united', 'states', 'first', 'amendment', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'surprisingly', 'and', 'wonderfully', 'light', 'hearted', 'and', 'humourous', 'much', 'of', 'the', 'comedy', 'is', 'elicited', 'from', 'larry', 'flynt', 's', 'outlandish', 'stunts', 'at', 'his', 'courtroom', 'appearances', 'some', 'of', 'his', 'chosen', 'apparel', 'is', 'hilarious', 'and', 'for', 'the', 'most', 'part', 'these', 'elements', 'of', 'the', 'film', 'work', 'far', 'better', 'than', 'some', 'of', 'the', 'more', 'dramatic', 'points', 'such', 'as', 'an', 'uninspiring', 'flynt', 'monologue', 'set', 'at', 'a', 'free', 'speech', 'rally', 'in', 'front', 'of', 'an', 'enormous', 'american', 'flag', 'dealing', 'with', 'the', 'subjectivity', 'of', 'obscenity', 'the', 'film', 's', 'focus', 'is', 'on', 'the', 'flynt', 's', 'many', 'battles', 'over', 'first', 'amendment', 'rights', 'and', 'freedom', 'of', 'speech', 'but', 'the', 'heart', 'of', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'the', 'touching', 'love', 'story', 'between', 'flynt', 'and', 'althea', 'larry', 'flynt', 'is', 'shown', 'as', 'being', 'occassionally', 'gruff', 'harsh', 'and', 'overtly', 'aggressive', 'with', 'his', 'friends', 'and', 'colleagues', 'but', 'with', 'althea', 'we', 'see', 'his', 'loving', 'affectionate', 'side', 'there', 's', 'a', 'scene', 'where', 'flynt', 'tenderly', 'takes', 'his', 'ill', 'wife', 'on', 'a', 'ride', 'on', 'his', 'wheelchair', 'that', 'is', 'heartbreaking', 'ultimately', 'the', 'emotional', 'power', 'that', 'the', 'film', 'hits', 'at', 'its', 'conclusion', 'comes', 'not', 'from', 'his', 'achievements', 'from', 'his', 'battles', 'against', 'censors', 'but', 'from', 'the', 'strength', 'of', 'flynt', 'and', 'althea', 's', 'love', 'for', 'each', 'other', 'woody', 'harrelson', 'is', 'entirely', 'engaging', 'in', 'what', 'must', 'be', 'certainly', 'a', 'career', 'topping', 'performance', 'as', 'the', 'irrepressible', 'larry', 'flynt', 'harrelson', 'plays', 'flynt', 'with', 'the', 'right', 'mixture', 'of', 'outrageousness', 'and', 'confident', 'stubborness', 'to', 'make', 'him', 'endearing', 'and', 'entirely', 'sympathetic', 'to', 'the', 'audience', 'and', 'a', 'very', 'compelling', 'protagonist', 'for', 'the', 'film', 'courtney', 'love', 'plays', 'althea', 'leasure', 'in', 'a', 'startling', 'turn', 'completely', 'raw', 'and', 'impulsive', 'it', 's', 'a', 'very', 'solid', 'performance', 'brash', 'and', 'naturalistic', 'and', 'love', 'is', 'extremely', 'compelling', 'it', 's', 'difficult', 'to', 'take', 'your', 'eyes', 'off', 'her', 'onscreen', 'and', 'her', 'chemistry', 'with', 'harrelson', 'is', 'dead', 'on', 'edward', 'norton', 'as', 'flynt', 's', 'straight', 'level', 'headed', 'lawyer', 'is', 'often', 'upstaged', 'by', 'his', 'flashier', 'co', 'stars', 'in', 'the', 'people', 'vs', 'larry', 'flynt', 'much', 'as', 'his', 'counterpart', 'lawyer', 'alan', 'isaacman', 'was', 'upstaged', 'by', 'flynt', 'during', 'many', 'of', 'the', 'courtroom', 'scenes', 'but', 'norton', 'shines', 'in', 'his', 'big', 'scene', 'where', 'he', 'addresses', 'the', 'supreme', 'court', 'in', 'the', 'climactic', 'scene', 'of', 'the', 'film', 'one', 'can', 'sense', 'the', 'frustration', 'that', 'norton', 's', 'character', 'feels', 'when', 'harrelson', 's', 'free', 'talking', 'flynt', 'sabotages', 'trial', 'after', 'trial', 'on', 'him', 'by', 'openly', 'speaking', 'his', 'mind', 'and', 'this', 'results', 'in', 'a', 'heightened', 'emotional', 'punch', 'when', 'norton', 's', 'isaacman', 'has', 'the', 'opportunity', 'to', 'sway', 'the', 'supreme', 'court', 'judges', 'milos', 'forman', 'keeps', 'the', 'film', 'moving', 'although', 'it', 'runs', 'over', 'two', 'hours', 'it', 'never', 'drags', 'and', 'his', 'direction', 'of', 'the', 'film', 'is', 'very', 'effective', 'eliciting', 'a', 'great', 'deal', 'of', 'empathy', 'for', 'a', 'subject', 'which', 'could', 'be', 'construed', 'by', 'some', 'as', 'extremely', 'sordid', 'and', 'unsympathetic', 'there', 's', 'also', 'a', 'great', 'visual', 'technique', 'which', 'forman', 'uses', 'to', 'indicate', 'the', 'passing', 'of', 'time', 'in', 'one', 'shot', 'which', 'is', 'both', 'clever', 'and', 'extremely', 'entertaining', 'two', 'minor', 'quibbles', 'with', 'the', 'film', 'it', 'certainly', 'seems', 'like', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'in', 'a', 'rush', 'to', 'get', 'to', 'its', 'main', 'theme', 'with', 'flynt', 'battling', 'against', 'authorities', 'and', 'the', 'system', 'for', 'his', 'freedom', 'of', 'speech', 'consequently', 'the', 'first', 'thirty', 'minutes', 'of', 'the', 'film', 'introducing', 'and', 'setting', 'up', 'the', 'characters', 'seem', 'unduly', 'rushed', 'perhaps', 'it', 'is', 'merely', 'due', 'to', 'the', 'fact', 'that', 'these', 'characters', 'are', 'so', 'interesting', 'but', 'i', 'felt', 'it', 'would', 'have', 'worked', 'better', 'if', 'this', 'route', 'was', 'taken', 'in', 'a', 'more', 'leisurely', 'fashion', 'it', 'also', 'felt', 'like', 'there', 'was', 'a', 'distinctive', 'lack', 'of', 'insight', 'into', 'the', 'inner', 'workings', 'of', 'these', 'characters', 'the', 'film', 'clearly', 'shows', 'what', 'flynt', 'althea', 'isaacman', 'and', 'rev', 'jerry', 'faldwell', 'did', 'and', 'on', 'a', 'superficial', 'level', 'some', 'of', 'their', 'motivations', 'but', 'it', 'never', 'seemed', 'like', 'one', 'could', 'really', 'understand', 'the', 'characters', 'on', 'a', 'deeper', 'level', 'for', 'example', 'why', 'larry', 'flynt', 'was', 'compelled', 'by', 'ruth', 'carter', 'stapleton', 'president', 'carter', 's', 'sister', 'to', 'be', 'born', 'again', 'is', 'a', 'mystery', 'to', 'me', 'then', 'again', 'perhaps', 'it', 'was', 'to', 'him', 'as', 'well', 'these', 'two', 'points', 'don', 't', 'detract', 'greatly', 'from', 'the', 'film', 'the', 'people', 'vs', 'larry', 'flynt', 'is', 'certainly', 'among', 'the', 'very', 'best', 'studio', 'released', 'films', 'of', '1996', 'and', 'works', 'both', 'as', 'a', 'ringing', 'political', 'statement', 'about', 'the', 'importance', 'of', 'freedom', 'of', 'speech', 'and', 'the', 'depths', 'to', 'which', 'larry', 'flynt', 'would', 'go', 'to', 'advance', 'the', 'cause', 'of', 'free', 'expression', 'and', 'as', 'a', 'touching', 'love', 'story']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-375-89232f130b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mnumberOfRuntimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumberOfRuntimes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mNBscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLRscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowerCase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlaplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposPrior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0maccuracyNB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNBscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mf1NB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNBscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-374-0493873adcbb>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(dataset, stemming, lowerCase, laplace, posPrior)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mpredictedLabelsNB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlaplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposPrior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpredictedLabelsLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mpredictedLabelsSVM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msupportVectorMachine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mpredictedLabelsDT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-367-c0b8f06f4b07>\u001b[0m in \u001b[0;36mlogisticRegression\u001b[1;34m(train_set, train_labels, dev_set)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mLRclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = \"../TermProject/txt_sentoken\"\n",
    "    stemming = False\n",
    "    lowerCase = True\n",
    "    laplace = 1.0\n",
    "    posPrior = 0.8\n",
    "    \n",
    "    ##Naive bayes\n",
    "    accuracyNB = []\n",
    "    f1NB = []\n",
    "    precisionNB = []\n",
    "    recallNB = []\n",
    "    \n",
    "    #Logistic regression\n",
    "    accuracyLR = []\n",
    "    f1LR = []\n",
    "    precisionLR = []\n",
    "    recallLR = []\n",
    "    \n",
    "    #Support Vector Machine\n",
    "    accuracySVM = []\n",
    "    f1SVM = []\n",
    "    precisionSVM = []\n",
    "    recallSVM = []\n",
    "    \n",
    "    #Decision Tree\n",
    "    accuracyDT = []\n",
    "    f1DT = []\n",
    "    precisionDT = []\n",
    "    recallDT = []\n",
    "    \n",
    "    numberOfRuntimes = 5\n",
    "    for i in range(numberOfRuntimes):\n",
    "        NBscores, LRscores, SVMscores = main(dataset, stemming, lowerCase, laplace, posPrior)\n",
    "        accuracyNB.append(NBscores[0])\n",
    "        f1NB.append(NBscores[1])\n",
    "        precisionNB.append(NBscores[2])\n",
    "        recallNB.append(NBscores[3])\n",
    "        \n",
    "        accuracyLR.append(LRscores[0])\n",
    "        f1LR.append(LRscores[1])\n",
    "        precisionLR.append(LRscores[2])\n",
    "        recallLR.append(LRscores[3])\n",
    "        \n",
    "        accuracySVM.append(SVMscores[0])\n",
    "        f1SVM.append(SVMscores[1])\n",
    "        precisionSVM.append(SVMscores[2])\n",
    "        recallSVM.append(SVMscores[3])\n",
    "        \n",
    "        accuracyDT.append(DTscores[0])\n",
    "        f1DT.append(DTscores[1])\n",
    "        precisionDT.append(DTscores[2])\n",
    "        recallDT.append(DTscores[3])\n",
    "        \n",
    "#         print(\"RUN NUMBER \" + str(i+1) + \" ---------------\")\n",
    "#         print(\"Accuracy:\",curaccuracy)\n",
    "#         print(\"F1-Score:\",curf1)\n",
    "#         print(\"Precision:\",curprecision)\n",
    "#         print(\"Recall:\",currecall)\n",
    "\n",
    "    #RESULTS OF NAIVE BAYES (unigram) \n",
    "    aveAccuracy = np.mean(accuracyNB)\n",
    "    avef1 = np.mean(f1NB)\n",
    "    avePrecision = np.mean(precisionNB)\n",
    "    aveRecall = np.mean(recallNB)\n",
    "    stdAccuracy = np.std(accuracyNB)\n",
    "    stdf1 = np.std(f1NB)\n",
    "    stdPrecision = np.std(precisionNB)\n",
    "    stdRecall = np.std(recallNB)\n",
    "    print(\"Final results NAIVE BAYES----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF LOGISTIC REGRESSION\n",
    "    aveAccuracy = np.mean(accuracyLR)\n",
    "    avef1 = np.mean(f1LR)\n",
    "    avePrecision = np.mean(precisionLR)\n",
    "    aveRecall = np.mean(recallLR)\n",
    "    stdAccuracy = np.std(accuracyLR)\n",
    "    stdf1 = np.std(f1LR)\n",
    "    stdPrecision = np.std(precisionLR)\n",
    "    stdRecall = np.std(recallLR)\n",
    "    print(\"Final results  LOGISTIC REGRESSION----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF SUPPORT VECTOR MACHINE\n",
    "    aveAccuracy = np.mean(accuracySVM)\n",
    "    avef1 = np.mean(f1SVM)\n",
    "    avePrecision = np.mean(precisionSVM)\n",
    "    aveRecall = np.mean(recallSVM)\n",
    "    stdAccuracy = np.std(accuracySVM)\n",
    "    stdf1 = np.std(f1SVM)\n",
    "    stdPrecision = np.std(precisionSVM)\n",
    "    stdRecall = np.std(recallSVM)\n",
    "    print(\"Final results SUPPORT VECTOR MACHINE----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)\n",
    "    \n",
    "    #RESULTS OF DECISION TREE\n",
    "    aveAccuracy = np.mean(accuracyDT)\n",
    "    avef1 = np.mean(f1DT)\n",
    "    avePrecision = np.mean(precisionDT)\n",
    "    aveRecall = np.mean(recallDT)\n",
    "    stdAccuracy = np.std(accuracyDT)\n",
    "    stdf1 = np.std(f1DT)\n",
    "    stdPrecision = np.std(precisionDT)\n",
    "    stdRecall = np.std(recallDT)\n",
    "    print(\"Final results DECISION TREE----------------------------------\")\n",
    "    print(\"Average Accuracy:\", aveAccuracy)\n",
    "    print(\"Average F1:\", avef1)\n",
    "    print(\"Average Precision:\", avePrecision)\n",
    "    print(\"Average recall\", aveRecall)\n",
    "    print(\"STD Accuracy:\", stdAccuracy)\n",
    "    print(\"STD F1:\", stdf1)\n",
    "    print(\"STD Precision:\", stdPrecision)\n",
    "    print(\"STD Recall:\", stdRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
